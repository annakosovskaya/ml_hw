{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYp0bXOFK-hP"
      },
      "source": [
        "# Машинное обучение, ФКН ВШЭ\n",
        "\n",
        "## Практическое задание 8. Метод опорных векторов и аппроксимация ядер\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY8vT0W_K-hR"
      },
      "source": [
        "### О задании\n",
        "\n",
        "На занятиях мы подробно обсуждали метод опорных векторов (SVM). В базовой версии в нём нет чего-то особенного — мы всего лишь используем специальную функцию потерь, которая не требует устремлять отступы к бесконечности; ей достаточно, чтобы отступы были не меньше +1. Затем мы узнали, что SVM можно переписать в двойственном виде, который, позволяет заменить скалярные произведения объектов на ядра. Это будет соответствовать построению модели в новом пространстве более высокой размерности, координаты которого представляют собой нелинейные модификации исходных признаков.\n",
        "\n",
        "Ядровой SVM, к сожалению, довольно затратен по памяти (нужно хранить матрицу Грама размера $d \\times d$) и по времени (нужно решать задачу условной оптимизации с квадратичной функцией, а это не очень быстро). Мы обсуждали, что есть способы посчитать новые признаки $\\tilde \\varphi(x)$ на основе исходных так, что скалярные произведения этих новых $\\langle \\tilde \\varphi(x), \\tilde \\varphi(z) \\rangle$ приближают ядро $K(x, z)$.\n",
        "\n",
        "Мы будем исследовать аппроксимации методом Random Fourier Features (RFF, также в литературе встречается название Random Kitchen Sinks) для гауссовых ядер. Будем использовать формулы, которые немного отличаются от того, что было на лекциях (мы добавим сдвиги внутрь тригонометрических функций и будем использовать только косинусы, потому что с нужным сдвигом косинус превратится в синус):\n",
        "$$\\tilde \\varphi(x) = (\n",
        "\\cos (w_1^T x + b_1),\n",
        "\\dots,\n",
        "\\cos (w_n^T x + b_n)\n",
        "),$$\n",
        "где $w_j \\sim \\mathcal{N}(0, 1/\\sigma^2)$, $b_j \\sim U[-\\pi, \\pi]$.\n",
        "\n",
        "На новых признаках $\\tilde \\varphi(x)$ мы будем строить любую линейную модель.\n",
        "\n",
        "Можно считать, что это некоторая новая парадигма построения сложных моделей. Можно направленно искать сложные нелинейные закономерности в данных с помощью градиентного бустинга или нейронных сетей, а можно просто нагенерировать большое количество случайных нелинейных признаков и надеяться, что быстрая и простая модель (то есть линейная) сможет показать на них хорошее качество. В этом задании мы изучим, насколько работоспособна такая идея.\n",
        "\n",
        "### Алгоритм\n",
        "\n",
        "Вам потребуется реализовать следующий алгоритм:\n",
        "1. Понизить размерность выборки до new_dim с помощью метода главных компонент.\n",
        "2. Для полученной выборки оценить гиперпараметр $\\sigma^2$ с помощью эвристики (рекомендуем считать медиану не по всем парам объектов, а по случайному подмножеству из где-то миллиона пар объектов): $$\\sigma^2 = \\text{median}_{i, j = 1, \\dots, \\ell, i \\neq j} \\left\\{\\sum_{k = 1}^{d} (x_{ik} - x_{jk})^2 \\right\\}$$\n",
        "3. Сгенерировать n_features наборов весов $w_j$ и сдвигов $b_j$.\n",
        "4. Сформировать n_features новых признаков по формулам, приведённым выше.\n",
        "5. Обучить линейную модель (логистическую регрессию или SVM) на новых признаках.\n",
        "6. Повторить преобразования (PCA, формирование новых признаков) к тестовой выборке и применить модель."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_sGunb7K-hS"
      },
      "source": [
        "Тестировать алгоритм мы будем на данных Fashion MNIST. Ниже код для их загрузки и подготовки."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyG6dBfjK-hS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ceaf2b3-9772-4665-fc44-2ab0f1490559"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 1s 0us/step\n",
            "26435584/26421880 [==============================] - 1s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "(x_train_pics, y_train), (x_test_pics, y_test) = fashion_mnist.load_data()\n",
        "x_train = x_train_pics.reshape(y_train.shape[0], -1)\n",
        "x_test = x_test_pics.reshape(y_test.shape[0], -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJNN55F7K-hT"
      },
      "source": [
        "__Задание 1. (5 баллов)__\n",
        "\n",
        "Реализуйте алгоритм, описанный выше. Можете воспользоваться шаблоном класса ниже или написать свой интерфейс.\n",
        "\n",
        "Ваша реализация должна поддерживать следующие опции:\n",
        "1. Возможность задавать значения гиперпараметров new_dim (по умолчанию 50) и n_features (по умолчанию 1000).\n",
        "2. Возможность включать или выключать предварительное понижение размерности с помощью метода главных компонент.\n",
        "3. Возможность выбирать тип линейной модели (логистическая регрессия или SVM с линейным ядром).\n",
        "\n",
        "Протестируйте на данных Fashion MNIST, сформированных кодом выше. Если на тесте у вас получилась доля верных ответов не ниже 0.84 с гиперпараметрами по умолчанию, то вы всё сделали правильно."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mR8XhG-zk3Ma",
        "outputId": "54098851-59e3-4850-c47a-2f423a2175d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.0.4-cp37-none-manylinux1_x86_64.whl (76.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 76.1 MB 56 kB/s \n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (5.5.0)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.3.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (3.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->catboost) (0.11.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost) (8.0.1)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-1.0.4\n"
          ]
        }
      ],
      "source": [
        "pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZ5ilqnnEe9E"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jP8yepx8K-hT"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class RFFPipeline(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, n_features=1000, new_dim=50, use_PCA=True, classifier='logreg'):\n",
        "        \"\"\"        \n",
        "        Implements pipeline, which consists of PCA decomposition,\n",
        "        Random Fourier Features approximation and linear classification model.\n",
        "        \n",
        "        n_features, int: amount of synthetic random features generated with RFF approximation.\n",
        "\n",
        "        new_dim, int: PCA output size.\n",
        "        \n",
        "        use_PCA, bool: whether to include PCA preprocessing.\n",
        "        \n",
        "        classifier, string: either 'svm' or 'logreg', a linear classification model to use on top of pipeline.\n",
        "        \n",
        "        Feel free to edit this template for your preferences.    \n",
        "        \"\"\"\n",
        "        self.n_features = n_features\n",
        "        self.new_dim = new_dim\n",
        "\n",
        "        self.use_PCA = use_PCA\n",
        "        self.PCA = PCA(n_components=self.new_dim)\n",
        "\n",
        "        self.classifier = classifier\n",
        "        if self.classifier == 'logreg':\n",
        "            self.model = LogisticRegression()\n",
        "        else:\n",
        "            self.model = SVC(kernel=self.classifier)\n",
        "  \n",
        "    def sigma_2(self, X):\n",
        "        # получим миллион пар (i_k, j_k)\n",
        "        i = np.random.choice(X.shape[0], size=1000000)\n",
        "        j = np.random.choice(X.shape[0], size=1000000)\n",
        "\n",
        "        # но некоторые могут быть вида (i, i), их нельзя сравнивать. просто выкинем\n",
        "        # т.к X достаточно большой, сильно на числе пар это не отразится\n",
        "        i, j = i[i!=j], j[i!=j]\n",
        "\n",
        "        # результат\n",
        "        return np.median(np.sum((X[i] - X[j])**2, axis=1))\n",
        "\n",
        "    def use_PCA_func(self, X):\n",
        "        if self.use_PCA:\n",
        "            X = self.PCA.transform(X)\n",
        "        else:\n",
        "            self.new_dim = X.shape[1]\n",
        "        return X\n",
        "\n",
        "    def new_feat(self, X):\n",
        "      return np.cos(X.dot(self.w) + self.b)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit all parts of algorithm (PCA, RFF, Classification) to training set.\n",
        "        \"\"\"\n",
        "        self.PCA.fit(X)\n",
        "        X = self.use_PCA_func(X)\n",
        "\n",
        "        self.sigma = self.sigma_2(X)\n",
        "        self.w = np.random.normal(0, 1/np.sqrt(self.sigma), size=(self.new_dim, self.n_features))\n",
        "        self.b = np.random.uniform(-np.pi, np.pi, size=self.n_features)\n",
        "\n",
        "        new_X = self.new_feat(X)\n",
        "        self.model.fit(new_X, y)\n",
        "        \n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Apply pipeline to obtain scores for input data.\n",
        "        \"\"\"\n",
        "        X = self.use_PCA_func(X)\n",
        "        new_X = self.new_feat(X)\n",
        "        return self.model.predict_proba(new_X) \n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Apply pipeline to obtain discrete predictions for input data.\n",
        "        \"\"\"\n",
        "        X = self.use_PCA_func(X)\n",
        "        new_X = self.new_feat(X)\n",
        "        return self.model.predict(new_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lySTQXOCP64B",
        "outputId": "0102b41f-0b22-4e59-e1b0-33a209fb951c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8598"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# проверяем качество\n",
        "\n",
        "model = RFFPipeline()\n",
        "model.fit(x_train, y_train)\n",
        "accuracy_score(model.predict(x_test), y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYqQUEi-K-hU"
      },
      "source": [
        "__Задание 2. (3 балла)__\n",
        "\n",
        "Сравните подход со случайными признаками с обучением SVM на исходных признаках. Попробуйте вариант с обычным (линейным) SVM и с ядровым SVM. Ядровой SVM может очень долго обучаться, поэтому можно делать любые разумные вещи для ускорения: брать подмножество объектов из обучающей выборки, например.\n",
        "\n",
        "Сравните подход со случайными признаками с вариантом, в котором вы понижаете размерность с помощью PCA и обучаете градиентный бустинг. Используйте одну из реализаций CatBoost/LightGBM/XGBoost, не забудьте подобрать число деревьев и длину шага.\n",
        "\n",
        "Сделайте выводы — насколько идея со случайными признаками работает? Сравните как с точки зрения качества, так и с точки зрения скорости обучения и применения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjWaTMFYlsTH"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "\n",
        "x_train_loc = x_train[len(x_train)//2:]\n",
        "y_train_loc = y_train[len(y_train)//2:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trXIlcoG00xL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26f624ec-c64d-4f28-bcec-c07d41acc905"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Времени на обучение линейного SVM с случайными признаками: 695.3424611091614\n",
            "Accuracy: 0.8764\n"
          ]
        }
      ],
      "source": [
        "# случайные признаки линейное SVM\n",
        "\n",
        "beg = time.time()\n",
        "\n",
        "svm_linear_rand = RFFPipeline(classifier='linear')\n",
        "svm_linear_rand.fit(x_train, y_train)\n",
        "\n",
        "print(\"Времени на обучение линейного SVM с случайными признаками:\", time.time() - beg)\n",
        "print(\"Accuracy:\", accuracy_score(svm_linear_rand.predict(x_test), y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qN8LUlJgK-hV",
        "outputId": "0f869984-47ba-4eb4-b976-4cfd6fa870e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Времени на обучение линейного SVM: 322.5119731426239\n",
            "Accuracy: 0.819\n"
          ]
        }
      ],
      "source": [
        "# линейное SVM\n",
        "\n",
        "beg = time.time()\n",
        "\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(x_train_loc, y_train_loc)\n",
        "\n",
        "print(\"Времени на обучение линейного SVM:\", time.time() - beg)\n",
        "print(\"Accuracy:\", accuracy_score(svm_linear.predict(x_test), y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подход с случайными признаками дает значительно лучшее качество (и в целом оно неплохое), но работает в два раза дольше. Если у нас есть достаточно времени, то случайны признаки --- тут хороший вариант."
      ],
      "metadata": {
        "id": "hPbUVN-shhPY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-aWpah9xrgt",
        "outputId": "f3c417c7-6dee-4e2a-95a3-dcecf2852b34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Времени потребовалось на обучение ярового SVM с случайными признаками: 209.55411887168884\n",
            "Accuracy: 0.8525\n"
          ]
        }
      ],
      "source": [
        "# случайные признаки SVM ядровой\n",
        "\n",
        "beg = time.time()\n",
        "\n",
        "svm_rbf_rand = RFFPipeline(classifier='rbf')\n",
        "svm_rbf_rand.fit(x_train_loc, y_train_loc)\n",
        "\n",
        "print(\"Времени потребовалось на обучение ярового SVM с случайными признаками:\", time.time() - beg)\n",
        "print(\"Accuracy:\", accuracy_score(svm_rbf_rand.predict(x_test), y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RVFpSHiG18BT",
        "outputId": "08c863f2-aa8d-4b0e-fa59-58bdb7668e23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Времени потребовалось на обучение SVM ядрового: 254.82953596115112\n",
            "Accuracy: 0.8771\n"
          ]
        }
      ],
      "source": [
        "# SVM ядровой\n",
        "\n",
        "beg = time.time()\n",
        "\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(x_train_loc, y_train_loc)\n",
        "\n",
        "print(\"Времени потребовалось на обучение SVM ядрового:\", time.time() - beg)\n",
        "print(\"Accuracy:\", accuracy_score(svm_rbf.predict(x_test), y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Время работы и качество примерно одинаковые: ядровой SVM работает чуть лучше, но и чуть дольше. Разница некритичная, но смысла в нашем подходе, получается, нет."
      ],
      "metadata": {
        "id": "b_cQc7qwiNYp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd464b35-8e32-4430-f2fe-840a52d34900",
        "id": "N9e_BBDBia7q"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\tlearn: 1.9588269\ttotal: 260ms\tremaining: 3m 1s\n",
            "1:\tlearn: 1.7513725\ttotal: 511ms\tremaining: 2m 58s\n",
            "2:\tlearn: 1.6088840\ttotal: 760ms\tremaining: 2m 56s\n",
            "3:\tlearn: 1.4857346\ttotal: 1.01s\tremaining: 2m 56s\n",
            "4:\tlearn: 1.3876744\ttotal: 1.27s\tremaining: 2m 57s\n",
            "5:\tlearn: 1.3072378\ttotal: 1.53s\tremaining: 2m 56s\n",
            "6:\tlearn: 1.2393376\ttotal: 1.78s\tremaining: 2m 56s\n",
            "7:\tlearn: 1.1872977\ttotal: 2.03s\tremaining: 2m 55s\n",
            "8:\tlearn: 1.1307318\ttotal: 2.29s\tremaining: 2m 55s\n",
            "9:\tlearn: 1.0796910\ttotal: 2.54s\tremaining: 2m 55s\n",
            "10:\tlearn: 1.0379984\ttotal: 2.79s\tremaining: 2m 54s\n",
            "11:\tlearn: 0.9991147\ttotal: 3.05s\tremaining: 2m 54s\n",
            "12:\tlearn: 0.9684338\ttotal: 3.32s\tremaining: 2m 55s\n",
            "13:\tlearn: 0.9377863\ttotal: 3.57s\tremaining: 2m 55s\n",
            "14:\tlearn: 0.9073305\ttotal: 3.83s\tremaining: 2m 55s\n",
            "15:\tlearn: 0.8807186\ttotal: 4.1s\tremaining: 2m 55s\n",
            "16:\tlearn: 0.8595618\ttotal: 4.37s\tremaining: 2m 55s\n",
            "17:\tlearn: 0.8375598\ttotal: 4.63s\tremaining: 2m 55s\n",
            "18:\tlearn: 0.8176664\ttotal: 4.89s\tremaining: 2m 55s\n",
            "19:\tlearn: 0.8035617\ttotal: 5.14s\tremaining: 2m 54s\n",
            "20:\tlearn: 0.7876535\ttotal: 5.41s\tremaining: 2m 54s\n",
            "21:\tlearn: 0.7697867\ttotal: 5.68s\tremaining: 2m 55s\n",
            "22:\tlearn: 0.7551690\ttotal: 5.94s\tremaining: 2m 54s\n",
            "23:\tlearn: 0.7432369\ttotal: 6.19s\tremaining: 2m 54s\n",
            "24:\tlearn: 0.7290812\ttotal: 6.46s\tremaining: 2m 54s\n",
            "25:\tlearn: 0.7171594\ttotal: 6.71s\tremaining: 2m 54s\n",
            "26:\tlearn: 0.7056820\ttotal: 6.97s\tremaining: 2m 53s\n",
            "27:\tlearn: 0.6942170\ttotal: 7.23s\tremaining: 2m 53s\n",
            "28:\tlearn: 0.6819911\ttotal: 7.51s\tremaining: 2m 53s\n",
            "29:\tlearn: 0.6711296\ttotal: 7.77s\tremaining: 2m 53s\n",
            "30:\tlearn: 0.6620754\ttotal: 8.03s\tremaining: 2m 53s\n",
            "31:\tlearn: 0.6521680\ttotal: 8.28s\tremaining: 2m 52s\n",
            "32:\tlearn: 0.6433493\ttotal: 8.55s\tremaining: 2m 52s\n",
            "33:\tlearn: 0.6346224\ttotal: 8.8s\tremaining: 2m 52s\n",
            "34:\tlearn: 0.6285964\ttotal: 9.05s\tremaining: 2m 51s\n",
            "35:\tlearn: 0.6222409\ttotal: 9.29s\tremaining: 2m 51s\n",
            "36:\tlearn: 0.6140220\ttotal: 9.56s\tremaining: 2m 51s\n",
            "37:\tlearn: 0.6085167\ttotal: 9.81s\tremaining: 2m 50s\n",
            "38:\tlearn: 0.6027808\ttotal: 10.1s\tremaining: 2m 50s\n",
            "39:\tlearn: 0.5966937\ttotal: 10.3s\tremaining: 2m 50s\n",
            "40:\tlearn: 0.5915360\ttotal: 10.6s\tremaining: 2m 50s\n",
            "41:\tlearn: 0.5860862\ttotal: 10.9s\tremaining: 2m 50s\n",
            "42:\tlearn: 0.5799878\ttotal: 11.1s\tremaining: 2m 50s\n",
            "43:\tlearn: 0.5745198\ttotal: 11.4s\tremaining: 2m 49s\n",
            "44:\tlearn: 0.5697270\ttotal: 11.7s\tremaining: 2m 49s\n",
            "45:\tlearn: 0.5646746\ttotal: 11.9s\tremaining: 2m 49s\n",
            "46:\tlearn: 0.5598024\ttotal: 12.2s\tremaining: 2m 49s\n",
            "47:\tlearn: 0.5556069\ttotal: 12.5s\tremaining: 2m 49s\n",
            "48:\tlearn: 0.5509042\ttotal: 12.8s\tremaining: 2m 49s\n",
            "49:\tlearn: 0.5453227\ttotal: 13s\tremaining: 2m 49s\n",
            "50:\tlearn: 0.5415483\ttotal: 13.3s\tremaining: 2m 49s\n",
            "51:\tlearn: 0.5370270\ttotal: 13.5s\tremaining: 2m 48s\n",
            "52:\tlearn: 0.5324261\ttotal: 13.8s\tremaining: 2m 48s\n",
            "53:\tlearn: 0.5286696\ttotal: 14.1s\tremaining: 2m 48s\n",
            "54:\tlearn: 0.5245639\ttotal: 14.3s\tremaining: 2m 47s\n",
            "55:\tlearn: 0.5209105\ttotal: 14.6s\tremaining: 2m 47s\n",
            "56:\tlearn: 0.5169238\ttotal: 14.8s\tremaining: 2m 47s\n",
            "57:\tlearn: 0.5130303\ttotal: 15.1s\tremaining: 2m 46s\n",
            "58:\tlearn: 0.5099479\ttotal: 15.3s\tremaining: 2m 46s\n",
            "59:\tlearn: 0.5061958\ttotal: 15.6s\tremaining: 2m 46s\n",
            "60:\tlearn: 0.5035603\ttotal: 15.9s\tremaining: 2m 46s\n",
            "61:\tlearn: 0.5004245\ttotal: 16.1s\tremaining: 2m 45s\n",
            "62:\tlearn: 0.4971862\ttotal: 16.4s\tremaining: 2m 45s\n",
            "63:\tlearn: 0.4940169\ttotal: 16.6s\tremaining: 2m 45s\n",
            "64:\tlearn: 0.4924514\ttotal: 16.9s\tremaining: 2m 45s\n",
            "65:\tlearn: 0.4893224\ttotal: 17.2s\tremaining: 2m 44s\n",
            "66:\tlearn: 0.4865017\ttotal: 17.4s\tremaining: 2m 44s\n",
            "67:\tlearn: 0.4833515\ttotal: 17.7s\tremaining: 2m 44s\n",
            "68:\tlearn: 0.4812678\ttotal: 17.9s\tremaining: 2m 44s\n",
            "69:\tlearn: 0.4784539\ttotal: 18.2s\tremaining: 2m 43s\n",
            "70:\tlearn: 0.4762116\ttotal: 18.4s\tremaining: 2m 43s\n",
            "71:\tlearn: 0.4732859\ttotal: 18.7s\tremaining: 2m 43s\n",
            "72:\tlearn: 0.4706386\ttotal: 19s\tremaining: 2m 43s\n",
            "73:\tlearn: 0.4684813\ttotal: 19.3s\tremaining: 2m 42s\n",
            "74:\tlearn: 0.4663994\ttotal: 19.5s\tremaining: 2m 42s\n",
            "75:\tlearn: 0.4638631\ttotal: 19.8s\tremaining: 2m 42s\n",
            "76:\tlearn: 0.4613676\ttotal: 20.1s\tremaining: 2m 42s\n",
            "77:\tlearn: 0.4589312\ttotal: 20.3s\tremaining: 2m 42s\n",
            "78:\tlearn: 0.4564396\ttotal: 20.6s\tremaining: 2m 41s\n",
            "79:\tlearn: 0.4546077\ttotal: 20.9s\tremaining: 2m 41s\n",
            "80:\tlearn: 0.4525103\ttotal: 21.2s\tremaining: 2m 42s\n",
            "81:\tlearn: 0.4500505\ttotal: 21.5s\tremaining: 2m 41s\n",
            "82:\tlearn: 0.4483461\ttotal: 21.7s\tremaining: 2m 41s\n",
            "83:\tlearn: 0.4462297\ttotal: 22s\tremaining: 2m 41s\n",
            "84:\tlearn: 0.4444518\ttotal: 22.3s\tremaining: 2m 41s\n",
            "85:\tlearn: 0.4427877\ttotal: 22.5s\tremaining: 2m 40s\n",
            "86:\tlearn: 0.4409644\ttotal: 22.8s\tremaining: 2m 40s\n",
            "87:\tlearn: 0.4389378\ttotal: 23s\tremaining: 2m 40s\n",
            "88:\tlearn: 0.4372279\ttotal: 23.3s\tremaining: 2m 39s\n",
            "89:\tlearn: 0.4357631\ttotal: 23.5s\tremaining: 2m 39s\n",
            "90:\tlearn: 0.4340956\ttotal: 23.8s\tremaining: 2m 39s\n",
            "91:\tlearn: 0.4324959\ttotal: 24.1s\tremaining: 2m 39s\n",
            "92:\tlearn: 0.4303808\ttotal: 24.3s\tremaining: 2m 38s\n",
            "93:\tlearn: 0.4288617\ttotal: 24.6s\tremaining: 2m 38s\n",
            "94:\tlearn: 0.4269573\ttotal: 24.9s\tremaining: 2m 38s\n",
            "95:\tlearn: 0.4255707\ttotal: 25.1s\tremaining: 2m 38s\n",
            "96:\tlearn: 0.4240004\ttotal: 25.4s\tremaining: 2m 37s\n",
            "97:\tlearn: 0.4225407\ttotal: 25.6s\tremaining: 2m 37s\n",
            "98:\tlearn: 0.4205132\ttotal: 25.9s\tremaining: 2m 37s\n",
            "99:\tlearn: 0.4195102\ttotal: 26.1s\tremaining: 2m 36s\n",
            "100:\tlearn: 0.4179808\ttotal: 26.4s\tremaining: 2m 36s\n",
            "101:\tlearn: 0.4171300\ttotal: 26.6s\tremaining: 2m 36s\n",
            "102:\tlearn: 0.4158154\ttotal: 26.9s\tremaining: 2m 35s\n",
            "103:\tlearn: 0.4150407\ttotal: 27.1s\tremaining: 2m 35s\n",
            "104:\tlearn: 0.4135879\ttotal: 27.4s\tremaining: 2m 35s\n",
            "105:\tlearn: 0.4122280\ttotal: 27.6s\tremaining: 2m 34s\n",
            "106:\tlearn: 0.4111175\ttotal: 27.9s\tremaining: 2m 34s\n",
            "107:\tlearn: 0.4094393\ttotal: 28.1s\tremaining: 2m 34s\n",
            "108:\tlearn: 0.4081345\ttotal: 28.4s\tremaining: 2m 34s\n",
            "109:\tlearn: 0.4071847\ttotal: 28.7s\tremaining: 2m 33s\n",
            "110:\tlearn: 0.4055771\ttotal: 28.9s\tremaining: 2m 33s\n",
            "111:\tlearn: 0.4040303\ttotal: 29.2s\tremaining: 2m 33s\n",
            "112:\tlearn: 0.4026717\ttotal: 29.4s\tremaining: 2m 32s\n",
            "113:\tlearn: 0.4014406\ttotal: 29.7s\tremaining: 2m 32s\n",
            "114:\tlearn: 0.4010414\ttotal: 29.9s\tremaining: 2m 32s\n",
            "115:\tlearn: 0.3999343\ttotal: 30.2s\tremaining: 2m 32s\n",
            "116:\tlearn: 0.3991597\ttotal: 30.4s\tremaining: 2m 31s\n",
            "117:\tlearn: 0.3984112\ttotal: 30.7s\tremaining: 2m 31s\n",
            "118:\tlearn: 0.3975511\ttotal: 31s\tremaining: 2m 31s\n",
            "119:\tlearn: 0.3966621\ttotal: 31.2s\tremaining: 2m 30s\n",
            "120:\tlearn: 0.3949195\ttotal: 31.5s\tremaining: 2m 30s\n",
            "121:\tlearn: 0.3942957\ttotal: 31.7s\tremaining: 2m 30s\n",
            "122:\tlearn: 0.3934833\ttotal: 32s\tremaining: 2m 30s\n",
            "123:\tlearn: 0.3925862\ttotal: 32.3s\tremaining: 2m 29s\n",
            "124:\tlearn: 0.3909883\ttotal: 32.5s\tremaining: 2m 29s\n",
            "125:\tlearn: 0.3899759\ttotal: 32.8s\tremaining: 2m 29s\n",
            "126:\tlearn: 0.3891508\ttotal: 33.1s\tremaining: 2m 29s\n",
            "127:\tlearn: 0.3885225\ttotal: 33.3s\tremaining: 2m 28s\n",
            "128:\tlearn: 0.3873783\ttotal: 33.6s\tremaining: 2m 28s\n",
            "129:\tlearn: 0.3868245\ttotal: 33.8s\tremaining: 2m 28s\n",
            "130:\tlearn: 0.3857653\ttotal: 34.1s\tremaining: 2m 28s\n",
            "131:\tlearn: 0.3844439\ttotal: 34.4s\tremaining: 2m 27s\n",
            "132:\tlearn: 0.3832000\ttotal: 34.6s\tremaining: 2m 27s\n",
            "133:\tlearn: 0.3826916\ttotal: 34.9s\tremaining: 2m 27s\n",
            "134:\tlearn: 0.3814645\ttotal: 35.2s\tremaining: 2m 27s\n",
            "135:\tlearn: 0.3808219\ttotal: 35.4s\tremaining: 2m 26s\n",
            "136:\tlearn: 0.3802456\ttotal: 35.7s\tremaining: 2m 26s\n",
            "137:\tlearn: 0.3791145\ttotal: 35.9s\tremaining: 2m 26s\n",
            "138:\tlearn: 0.3777904\ttotal: 36.2s\tremaining: 2m 26s\n",
            "139:\tlearn: 0.3768110\ttotal: 36.4s\tremaining: 2m 25s\n",
            "140:\tlearn: 0.3754906\ttotal: 36.7s\tremaining: 2m 25s\n",
            "141:\tlearn: 0.3749998\ttotal: 36.9s\tremaining: 2m 25s\n",
            "142:\tlearn: 0.3741322\ttotal: 37.2s\tremaining: 2m 24s\n",
            "143:\tlearn: 0.3731157\ttotal: 37.5s\tremaining: 2m 24s\n",
            "144:\tlearn: 0.3720175\ttotal: 37.7s\tremaining: 2m 24s\n",
            "145:\tlearn: 0.3711252\ttotal: 38s\tremaining: 2m 24s\n",
            "146:\tlearn: 0.3702370\ttotal: 38.2s\tremaining: 2m 23s\n",
            "147:\tlearn: 0.3696766\ttotal: 38.5s\tremaining: 2m 23s\n",
            "148:\tlearn: 0.3691718\ttotal: 38.7s\tremaining: 2m 23s\n",
            "149:\tlearn: 0.3688062\ttotal: 39s\tremaining: 2m 22s\n",
            "150:\tlearn: 0.3679594\ttotal: 39.2s\tremaining: 2m 22s\n",
            "151:\tlearn: 0.3669606\ttotal: 39.5s\tremaining: 2m 22s\n",
            "152:\tlearn: 0.3660081\ttotal: 39.7s\tremaining: 2m 22s\n",
            "153:\tlearn: 0.3648784\ttotal: 40s\tremaining: 2m 21s\n",
            "154:\tlearn: 0.3642924\ttotal: 40.2s\tremaining: 2m 21s\n",
            "155:\tlearn: 0.3634904\ttotal: 40.5s\tremaining: 2m 21s\n",
            "156:\tlearn: 0.3626732\ttotal: 40.7s\tremaining: 2m 20s\n",
            "157:\tlearn: 0.3619370\ttotal: 41s\tremaining: 2m 20s\n",
            "158:\tlearn: 0.3610728\ttotal: 41.2s\tremaining: 2m 20s\n",
            "159:\tlearn: 0.3601650\ttotal: 41.5s\tremaining: 2m 20s\n",
            "160:\tlearn: 0.3593985\ttotal: 41.7s\tremaining: 2m 19s\n",
            "161:\tlearn: 0.3586843\ttotal: 42s\tremaining: 2m 19s\n",
            "162:\tlearn: 0.3581620\ttotal: 42.3s\tremaining: 2m 19s\n",
            "163:\tlearn: 0.3571666\ttotal: 42.5s\tremaining: 2m 19s\n",
            "164:\tlearn: 0.3559426\ttotal: 42.8s\tremaining: 2m 18s\n",
            "165:\tlearn: 0.3552813\ttotal: 43.1s\tremaining: 2m 18s\n",
            "166:\tlearn: 0.3547241\ttotal: 43.3s\tremaining: 2m 18s\n",
            "167:\tlearn: 0.3541408\ttotal: 43.6s\tremaining: 2m 17s\n",
            "168:\tlearn: 0.3534420\ttotal: 43.8s\tremaining: 2m 17s\n",
            "169:\tlearn: 0.3522776\ttotal: 44.1s\tremaining: 2m 17s\n",
            "170:\tlearn: 0.3515622\ttotal: 44.3s\tremaining: 2m 17s\n",
            "171:\tlearn: 0.3509860\ttotal: 44.6s\tremaining: 2m 16s\n",
            "172:\tlearn: 0.3503430\ttotal: 44.8s\tremaining: 2m 16s\n",
            "173:\tlearn: 0.3499162\ttotal: 45.1s\tremaining: 2m 16s\n",
            "174:\tlearn: 0.3494112\ttotal: 45.4s\tremaining: 2m 16s\n",
            "175:\tlearn: 0.3486793\ttotal: 45.6s\tremaining: 2m 15s\n",
            "176:\tlearn: 0.3479690\ttotal: 45.9s\tremaining: 2m 15s\n",
            "177:\tlearn: 0.3472048\ttotal: 46.1s\tremaining: 2m 15s\n",
            "178:\tlearn: 0.3466616\ttotal: 46.4s\tremaining: 2m 15s\n",
            "179:\tlearn: 0.3460627\ttotal: 46.6s\tremaining: 2m 14s\n",
            "180:\tlearn: 0.3455060\ttotal: 46.9s\tremaining: 2m 14s\n",
            "181:\tlearn: 0.3446088\ttotal: 47.1s\tremaining: 2m 14s\n",
            "182:\tlearn: 0.3442121\ttotal: 47.4s\tremaining: 2m 13s\n",
            "183:\tlearn: 0.3435703\ttotal: 47.6s\tremaining: 2m 13s\n",
            "184:\tlearn: 0.3426682\ttotal: 47.9s\tremaining: 2m 13s\n",
            "185:\tlearn: 0.3422474\ttotal: 48.2s\tremaining: 2m 13s\n",
            "186:\tlearn: 0.3416870\ttotal: 48.4s\tremaining: 2m 12s\n",
            "187:\tlearn: 0.3410595\ttotal: 48.7s\tremaining: 2m 12s\n",
            "188:\tlearn: 0.3402874\ttotal: 48.9s\tremaining: 2m 12s\n",
            "189:\tlearn: 0.3394112\ttotal: 49.2s\tremaining: 2m 12s\n",
            "190:\tlearn: 0.3386650\ttotal: 49.5s\tremaining: 2m 11s\n",
            "191:\tlearn: 0.3380216\ttotal: 49.7s\tremaining: 2m 11s\n",
            "192:\tlearn: 0.3370593\ttotal: 50s\tremaining: 2m 11s\n",
            "193:\tlearn: 0.3364870\ttotal: 50.2s\tremaining: 2m 11s\n",
            "194:\tlearn: 0.3356957\ttotal: 50.5s\tremaining: 2m 10s\n",
            "195:\tlearn: 0.3352128\ttotal: 50.8s\tremaining: 2m 10s\n",
            "196:\tlearn: 0.3347600\ttotal: 51s\tremaining: 2m 10s\n",
            "197:\tlearn: 0.3343415\ttotal: 51.3s\tremaining: 2m 9s\n",
            "198:\tlearn: 0.3339828\ttotal: 51.5s\tremaining: 2m 9s\n",
            "199:\tlearn: 0.3334168\ttotal: 51.8s\tremaining: 2m 9s\n",
            "200:\tlearn: 0.3325780\ttotal: 52s\tremaining: 2m 9s\n",
            "201:\tlearn: 0.3319089\ttotal: 52.3s\tremaining: 2m 8s\n",
            "202:\tlearn: 0.3310901\ttotal: 52.5s\tremaining: 2m 8s\n",
            "203:\tlearn: 0.3303157\ttotal: 52.8s\tremaining: 2m 8s\n",
            "204:\tlearn: 0.3297556\ttotal: 53s\tremaining: 2m 8s\n",
            "205:\tlearn: 0.3291840\ttotal: 53.3s\tremaining: 2m 7s\n",
            "206:\tlearn: 0.3282652\ttotal: 53.5s\tremaining: 2m 7s\n",
            "207:\tlearn: 0.3277399\ttotal: 53.8s\tremaining: 2m 7s\n",
            "208:\tlearn: 0.3272082\ttotal: 54s\tremaining: 2m 6s\n",
            "209:\tlearn: 0.3268471\ttotal: 54.3s\tremaining: 2m 6s\n",
            "210:\tlearn: 0.3264445\ttotal: 54.5s\tremaining: 2m 6s\n",
            "211:\tlearn: 0.3258154\ttotal: 54.8s\tremaining: 2m 6s\n",
            "212:\tlearn: 0.3253451\ttotal: 55.1s\tremaining: 2m 5s\n",
            "213:\tlearn: 0.3247375\ttotal: 55.3s\tremaining: 2m 5s\n",
            "214:\tlearn: 0.3242771\ttotal: 55.5s\tremaining: 2m 5s\n",
            "215:\tlearn: 0.3233340\ttotal: 55.8s\tremaining: 2m 5s\n",
            "216:\tlearn: 0.3231064\ttotal: 56.1s\tremaining: 2m 4s\n",
            "217:\tlearn: 0.3224802\ttotal: 56.3s\tremaining: 2m 4s\n",
            "218:\tlearn: 0.3221550\ttotal: 56.6s\tremaining: 2m 4s\n",
            "219:\tlearn: 0.3213332\ttotal: 56.8s\tremaining: 2m 4s\n",
            "220:\tlearn: 0.3208386\ttotal: 57.1s\tremaining: 2m 3s\n",
            "221:\tlearn: 0.3199645\ttotal: 57.4s\tremaining: 2m 3s\n",
            "222:\tlearn: 0.3194725\ttotal: 57.6s\tremaining: 2m 3s\n",
            "223:\tlearn: 0.3190508\ttotal: 57.9s\tremaining: 2m 2s\n",
            "224:\tlearn: 0.3182286\ttotal: 58.1s\tremaining: 2m 2s\n",
            "225:\tlearn: 0.3177058\ttotal: 58.4s\tremaining: 2m 2s\n",
            "226:\tlearn: 0.3172154\ttotal: 58.6s\tremaining: 2m 2s\n",
            "227:\tlearn: 0.3166998\ttotal: 58.9s\tremaining: 2m 1s\n",
            "228:\tlearn: 0.3164496\ttotal: 59.1s\tremaining: 2m 1s\n",
            "229:\tlearn: 0.3156573\ttotal: 59.4s\tremaining: 2m 1s\n",
            "230:\tlearn: 0.3149784\ttotal: 59.6s\tremaining: 2m 1s\n",
            "231:\tlearn: 0.3141138\ttotal: 59.9s\tremaining: 2m\n",
            "232:\tlearn: 0.3132337\ttotal: 1m\tremaining: 2m\n",
            "233:\tlearn: 0.3127942\ttotal: 1m\tremaining: 2m\n",
            "234:\tlearn: 0.3123343\ttotal: 1m\tremaining: 2m\n",
            "235:\tlearn: 0.3118541\ttotal: 1m\tremaining: 1m 59s\n",
            "236:\tlearn: 0.3110768\ttotal: 1m 1s\tremaining: 1m 59s\n",
            "237:\tlearn: 0.3103406\ttotal: 1m 1s\tremaining: 1m 59s\n",
            "238:\tlearn: 0.3097222\ttotal: 1m 1s\tremaining: 1m 59s\n",
            "239:\tlearn: 0.3091629\ttotal: 1m 2s\tremaining: 1m 58s\n",
            "240:\tlearn: 0.3086995\ttotal: 1m 2s\tremaining: 1m 58s\n",
            "241:\tlearn: 0.3077864\ttotal: 1m 2s\tremaining: 1m 58s\n",
            "242:\tlearn: 0.3071215\ttotal: 1m 2s\tremaining: 1m 58s\n",
            "243:\tlearn: 0.3067603\ttotal: 1m 3s\tremaining: 1m 57s\n",
            "244:\tlearn: 0.3064141\ttotal: 1m 3s\tremaining: 1m 57s\n",
            "245:\tlearn: 0.3057926\ttotal: 1m 3s\tremaining: 1m 57s\n",
            "246:\tlearn: 0.3054747\ttotal: 1m 3s\tremaining: 1m 57s\n",
            "247:\tlearn: 0.3049776\ttotal: 1m 4s\tremaining: 1m 56s\n",
            "248:\tlearn: 0.3044659\ttotal: 1m 4s\tremaining: 1m 56s\n",
            "249:\tlearn: 0.3039366\ttotal: 1m 4s\tremaining: 1m 56s\n",
            "250:\tlearn: 0.3035499\ttotal: 1m 4s\tremaining: 1m 56s\n",
            "251:\tlearn: 0.3030335\ttotal: 1m 5s\tremaining: 1m 55s\n",
            "252:\tlearn: 0.3024953\ttotal: 1m 5s\tremaining: 1m 55s\n",
            "253:\tlearn: 0.3023462\ttotal: 1m 5s\tremaining: 1m 55s\n",
            "254:\tlearn: 0.3017787\ttotal: 1m 5s\tremaining: 1m 55s\n",
            "255:\tlearn: 0.3011422\ttotal: 1m 6s\tremaining: 1m 54s\n",
            "256:\tlearn: 0.3005727\ttotal: 1m 6s\tremaining: 1m 54s\n",
            "257:\tlearn: 0.2999096\ttotal: 1m 6s\tremaining: 1m 54s\n",
            "258:\tlearn: 0.2993037\ttotal: 1m 6s\tremaining: 1m 54s\n",
            "259:\tlearn: 0.2988549\ttotal: 1m 7s\tremaining: 1m 53s\n",
            "260:\tlearn: 0.2984149\ttotal: 1m 7s\tremaining: 1m 53s\n",
            "261:\tlearn: 0.2980369\ttotal: 1m 7s\tremaining: 1m 53s\n",
            "262:\tlearn: 0.2976283\ttotal: 1m 7s\tremaining: 1m 52s\n",
            "263:\tlearn: 0.2973005\ttotal: 1m 8s\tremaining: 1m 52s\n",
            "264:\tlearn: 0.2969301\ttotal: 1m 8s\tremaining: 1m 52s\n",
            "265:\tlearn: 0.2962276\ttotal: 1m 8s\tremaining: 1m 52s\n",
            "266:\tlearn: 0.2959762\ttotal: 1m 9s\tremaining: 1m 51s\n",
            "267:\tlearn: 0.2956544\ttotal: 1m 9s\tremaining: 1m 51s\n",
            "268:\tlearn: 0.2950152\ttotal: 1m 9s\tremaining: 1m 51s\n",
            "269:\tlearn: 0.2944324\ttotal: 1m 9s\tremaining: 1m 51s\n",
            "270:\tlearn: 0.2939436\ttotal: 1m 10s\tremaining: 1m 50s\n",
            "271:\tlearn: 0.2936053\ttotal: 1m 10s\tremaining: 1m 50s\n",
            "272:\tlearn: 0.2927556\ttotal: 1m 10s\tremaining: 1m 50s\n",
            "273:\tlearn: 0.2925852\ttotal: 1m 10s\tremaining: 1m 50s\n",
            "274:\tlearn: 0.2920705\ttotal: 1m 11s\tremaining: 1m 49s\n",
            "275:\tlearn: 0.2917174\ttotal: 1m 11s\tremaining: 1m 49s\n",
            "276:\tlearn: 0.2910604\ttotal: 1m 11s\tremaining: 1m 49s\n",
            "277:\tlearn: 0.2908188\ttotal: 1m 11s\tremaining: 1m 49s\n",
            "278:\tlearn: 0.2906037\ttotal: 1m 12s\tremaining: 1m 48s\n",
            "279:\tlearn: 0.2902656\ttotal: 1m 12s\tremaining: 1m 48s\n",
            "280:\tlearn: 0.2898374\ttotal: 1m 12s\tremaining: 1m 48s\n",
            "281:\tlearn: 0.2892320\ttotal: 1m 12s\tremaining: 1m 47s\n",
            "282:\tlearn: 0.2887239\ttotal: 1m 13s\tremaining: 1m 47s\n",
            "283:\tlearn: 0.2884829\ttotal: 1m 13s\tremaining: 1m 47s\n",
            "284:\tlearn: 0.2880114\ttotal: 1m 13s\tremaining: 1m 47s\n",
            "285:\tlearn: 0.2877037\ttotal: 1m 13s\tremaining: 1m 46s\n",
            "286:\tlearn: 0.2874608\ttotal: 1m 14s\tremaining: 1m 46s\n",
            "287:\tlearn: 0.2871402\ttotal: 1m 14s\tremaining: 1m 46s\n",
            "288:\tlearn: 0.2864160\ttotal: 1m 14s\tremaining: 1m 46s\n",
            "289:\tlearn: 0.2859584\ttotal: 1m 14s\tremaining: 1m 45s\n",
            "290:\tlearn: 0.2857011\ttotal: 1m 15s\tremaining: 1m 45s\n",
            "291:\tlearn: 0.2854207\ttotal: 1m 15s\tremaining: 1m 45s\n",
            "292:\tlearn: 0.2852224\ttotal: 1m 15s\tremaining: 1m 45s\n",
            "293:\tlearn: 0.2849009\ttotal: 1m 15s\tremaining: 1m 44s\n",
            "294:\tlearn: 0.2842919\ttotal: 1m 16s\tremaining: 1m 44s\n",
            "295:\tlearn: 0.2837747\ttotal: 1m 16s\tremaining: 1m 44s\n",
            "296:\tlearn: 0.2831440\ttotal: 1m 16s\tremaining: 1m 44s\n",
            "297:\tlearn: 0.2828746\ttotal: 1m 16s\tremaining: 1m 43s\n",
            "298:\tlearn: 0.2825189\ttotal: 1m 17s\tremaining: 1m 43s\n",
            "299:\tlearn: 0.2823417\ttotal: 1m 17s\tremaining: 1m 43s\n",
            "300:\tlearn: 0.2820155\ttotal: 1m 17s\tremaining: 1m 43s\n",
            "301:\tlearn: 0.2817925\ttotal: 1m 17s\tremaining: 1m 42s\n",
            "302:\tlearn: 0.2810960\ttotal: 1m 18s\tremaining: 1m 42s\n",
            "303:\tlearn: 0.2808147\ttotal: 1m 18s\tremaining: 1m 42s\n",
            "304:\tlearn: 0.2804972\ttotal: 1m 18s\tremaining: 1m 42s\n",
            "305:\tlearn: 0.2802027\ttotal: 1m 19s\tremaining: 1m 41s\n",
            "306:\tlearn: 0.2797264\ttotal: 1m 19s\tremaining: 1m 41s\n",
            "307:\tlearn: 0.2794120\ttotal: 1m 19s\tremaining: 1m 41s\n",
            "308:\tlearn: 0.2789542\ttotal: 1m 19s\tremaining: 1m 40s\n",
            "309:\tlearn: 0.2786034\ttotal: 1m 20s\tremaining: 1m 40s\n",
            "310:\tlearn: 0.2779512\ttotal: 1m 20s\tremaining: 1m 40s\n",
            "311:\tlearn: 0.2774671\ttotal: 1m 20s\tremaining: 1m 40s\n",
            "312:\tlearn: 0.2769741\ttotal: 1m 20s\tremaining: 1m 39s\n",
            "313:\tlearn: 0.2764670\ttotal: 1m 21s\tremaining: 1m 39s\n",
            "314:\tlearn: 0.2762332\ttotal: 1m 21s\tremaining: 1m 39s\n",
            "315:\tlearn: 0.2758850\ttotal: 1m 21s\tremaining: 1m 39s\n",
            "316:\tlearn: 0.2754040\ttotal: 1m 21s\tremaining: 1m 38s\n",
            "317:\tlearn: 0.2747865\ttotal: 1m 22s\tremaining: 1m 38s\n",
            "318:\tlearn: 0.2744616\ttotal: 1m 22s\tremaining: 1m 38s\n",
            "319:\tlearn: 0.2741377\ttotal: 1m 22s\tremaining: 1m 38s\n",
            "320:\tlearn: 0.2736982\ttotal: 1m 22s\tremaining: 1m 37s\n",
            "321:\tlearn: 0.2732584\ttotal: 1m 23s\tremaining: 1m 37s\n",
            "322:\tlearn: 0.2728973\ttotal: 1m 23s\tremaining: 1m 37s\n",
            "323:\tlearn: 0.2724189\ttotal: 1m 23s\tremaining: 1m 37s\n",
            "324:\tlearn: 0.2719775\ttotal: 1m 23s\tremaining: 1m 36s\n",
            "325:\tlearn: 0.2717445\ttotal: 1m 24s\tremaining: 1m 36s\n",
            "326:\tlearn: 0.2712362\ttotal: 1m 24s\tremaining: 1m 36s\n",
            "327:\tlearn: 0.2705813\ttotal: 1m 24s\tremaining: 1m 36s\n",
            "328:\tlearn: 0.2702057\ttotal: 1m 25s\tremaining: 1m 35s\n",
            "329:\tlearn: 0.2695421\ttotal: 1m 25s\tremaining: 1m 35s\n",
            "330:\tlearn: 0.2691454\ttotal: 1m 25s\tremaining: 1m 35s\n",
            "331:\tlearn: 0.2686162\ttotal: 1m 25s\tremaining: 1m 35s\n",
            "332:\tlearn: 0.2681087\ttotal: 1m 26s\tremaining: 1m 34s\n",
            "333:\tlearn: 0.2678164\ttotal: 1m 26s\tremaining: 1m 34s\n",
            "334:\tlearn: 0.2674808\ttotal: 1m 26s\tremaining: 1m 34s\n",
            "335:\tlearn: 0.2670721\ttotal: 1m 26s\tremaining: 1m 34s\n",
            "336:\tlearn: 0.2667511\ttotal: 1m 27s\tremaining: 1m 33s\n",
            "337:\tlearn: 0.2664875\ttotal: 1m 27s\tremaining: 1m 33s\n",
            "338:\tlearn: 0.2659518\ttotal: 1m 27s\tremaining: 1m 33s\n",
            "339:\tlearn: 0.2655349\ttotal: 1m 27s\tremaining: 1m 33s\n",
            "340:\tlearn: 0.2650430\ttotal: 1m 28s\tremaining: 1m 32s\n",
            "341:\tlearn: 0.2647112\ttotal: 1m 28s\tremaining: 1m 32s\n",
            "342:\tlearn: 0.2644839\ttotal: 1m 28s\tremaining: 1m 32s\n",
            "343:\tlearn: 0.2641630\ttotal: 1m 29s\tremaining: 1m 32s\n",
            "344:\tlearn: 0.2639983\ttotal: 1m 29s\tremaining: 1m 31s\n",
            "345:\tlearn: 0.2634973\ttotal: 1m 29s\tremaining: 1m 31s\n",
            "346:\tlearn: 0.2631062\ttotal: 1m 29s\tremaining: 1m 31s\n",
            "347:\tlearn: 0.2624331\ttotal: 1m 30s\tremaining: 1m 31s\n",
            "348:\tlearn: 0.2621836\ttotal: 1m 30s\tremaining: 1m 30s\n",
            "349:\tlearn: 0.2618914\ttotal: 1m 30s\tremaining: 1m 30s\n",
            "350:\tlearn: 0.2614573\ttotal: 1m 30s\tremaining: 1m 30s\n",
            "351:\tlearn: 0.2610130\ttotal: 1m 31s\tremaining: 1m 30s\n",
            "352:\tlearn: 0.2604651\ttotal: 1m 31s\tremaining: 1m 29s\n",
            "353:\tlearn: 0.2602582\ttotal: 1m 31s\tremaining: 1m 29s\n",
            "354:\tlearn: 0.2599518\ttotal: 1m 32s\tremaining: 1m 29s\n",
            "355:\tlearn: 0.2595583\ttotal: 1m 32s\tremaining: 1m 29s\n",
            "356:\tlearn: 0.2591337\ttotal: 1m 32s\tremaining: 1m 28s\n",
            "357:\tlearn: 0.2588357\ttotal: 1m 32s\tremaining: 1m 28s\n",
            "358:\tlearn: 0.2583496\ttotal: 1m 33s\tremaining: 1m 28s\n",
            "359:\tlearn: 0.2577975\ttotal: 1m 33s\tremaining: 1m 28s\n",
            "360:\tlearn: 0.2574591\ttotal: 1m 33s\tremaining: 1m 27s\n",
            "361:\tlearn: 0.2571568\ttotal: 1m 33s\tremaining: 1m 27s\n",
            "362:\tlearn: 0.2569283\ttotal: 1m 34s\tremaining: 1m 27s\n",
            "363:\tlearn: 0.2565650\ttotal: 1m 34s\tremaining: 1m 27s\n",
            "364:\tlearn: 0.2563658\ttotal: 1m 34s\tremaining: 1m 26s\n",
            "365:\tlearn: 0.2561174\ttotal: 1m 34s\tremaining: 1m 26s\n",
            "366:\tlearn: 0.2557405\ttotal: 1m 35s\tremaining: 1m 26s\n",
            "367:\tlearn: 0.2551856\ttotal: 1m 35s\tremaining: 1m 26s\n",
            "368:\tlearn: 0.2549630\ttotal: 1m 35s\tremaining: 1m 25s\n",
            "369:\tlearn: 0.2546179\ttotal: 1m 36s\tremaining: 1m 25s\n",
            "370:\tlearn: 0.2544890\ttotal: 1m 36s\tremaining: 1m 25s\n",
            "371:\tlearn: 0.2538778\ttotal: 1m 36s\tremaining: 1m 25s\n",
            "372:\tlearn: 0.2536524\ttotal: 1m 36s\tremaining: 1m 24s\n",
            "373:\tlearn: 0.2531920\ttotal: 1m 37s\tremaining: 1m 24s\n",
            "374:\tlearn: 0.2527605\ttotal: 1m 37s\tremaining: 1m 24s\n",
            "375:\tlearn: 0.2524328\ttotal: 1m 37s\tremaining: 1m 24s\n",
            "376:\tlearn: 0.2522029\ttotal: 1m 37s\tremaining: 1m 23s\n",
            "377:\tlearn: 0.2519902\ttotal: 1m 38s\tremaining: 1m 23s\n",
            "378:\tlearn: 0.2514839\ttotal: 1m 38s\tremaining: 1m 23s\n",
            "379:\tlearn: 0.2511164\ttotal: 1m 38s\tremaining: 1m 23s\n",
            "380:\tlearn: 0.2506711\ttotal: 1m 38s\tremaining: 1m 22s\n",
            "381:\tlearn: 0.2503016\ttotal: 1m 39s\tremaining: 1m 22s\n",
            "382:\tlearn: 0.2502028\ttotal: 1m 39s\tremaining: 1m 22s\n",
            "383:\tlearn: 0.2499757\ttotal: 1m 39s\tremaining: 1m 22s\n",
            "384:\tlearn: 0.2497254\ttotal: 1m 39s\tremaining: 1m 21s\n",
            "385:\tlearn: 0.2496064\ttotal: 1m 40s\tremaining: 1m 21s\n",
            "386:\tlearn: 0.2489675\ttotal: 1m 40s\tremaining: 1m 21s\n",
            "387:\tlearn: 0.2486955\ttotal: 1m 40s\tremaining: 1m 21s\n",
            "388:\tlearn: 0.2485912\ttotal: 1m 41s\tremaining: 1m 20s\n",
            "389:\tlearn: 0.2483916\ttotal: 1m 41s\tremaining: 1m 20s\n",
            "390:\tlearn: 0.2480573\ttotal: 1m 41s\tremaining: 1m 20s\n",
            "391:\tlearn: 0.2475521\ttotal: 1m 41s\tremaining: 1m 20s\n",
            "392:\tlearn: 0.2472317\ttotal: 1m 42s\tremaining: 1m 19s\n",
            "393:\tlearn: 0.2468187\ttotal: 1m 42s\tremaining: 1m 19s\n",
            "394:\tlearn: 0.2462520\ttotal: 1m 42s\tremaining: 1m 19s\n",
            "395:\tlearn: 0.2458500\ttotal: 1m 42s\tremaining: 1m 19s\n",
            "396:\tlearn: 0.2455391\ttotal: 1m 43s\tremaining: 1m 18s\n",
            "397:\tlearn: 0.2452863\ttotal: 1m 43s\tremaining: 1m 18s\n",
            "398:\tlearn: 0.2450158\ttotal: 1m 43s\tremaining: 1m 18s\n",
            "399:\tlearn: 0.2445628\ttotal: 1m 43s\tremaining: 1m 17s\n",
            "400:\tlearn: 0.2441858\ttotal: 1m 44s\tremaining: 1m 17s\n",
            "401:\tlearn: 0.2437776\ttotal: 1m 44s\tremaining: 1m 17s\n",
            "402:\tlearn: 0.2434951\ttotal: 1m 44s\tremaining: 1m 17s\n",
            "403:\tlearn: 0.2433619\ttotal: 1m 45s\tremaining: 1m 16s\n",
            "404:\tlearn: 0.2430534\ttotal: 1m 45s\tremaining: 1m 16s\n",
            "405:\tlearn: 0.2428256\ttotal: 1m 45s\tremaining: 1m 16s\n",
            "406:\tlearn: 0.2425547\ttotal: 1m 45s\tremaining: 1m 16s\n",
            "407:\tlearn: 0.2421858\ttotal: 1m 46s\tremaining: 1m 15s\n",
            "408:\tlearn: 0.2419666\ttotal: 1m 46s\tremaining: 1m 15s\n",
            "409:\tlearn: 0.2416311\ttotal: 1m 46s\tremaining: 1m 15s\n",
            "410:\tlearn: 0.2412267\ttotal: 1m 46s\tremaining: 1m 15s\n",
            "411:\tlearn: 0.2409121\ttotal: 1m 47s\tremaining: 1m 14s\n",
            "412:\tlearn: 0.2407232\ttotal: 1m 47s\tremaining: 1m 14s\n",
            "413:\tlearn: 0.2404234\ttotal: 1m 47s\tremaining: 1m 14s\n",
            "414:\tlearn: 0.2400209\ttotal: 1m 47s\tremaining: 1m 14s\n",
            "415:\tlearn: 0.2397854\ttotal: 1m 48s\tremaining: 1m 13s\n",
            "416:\tlearn: 0.2395710\ttotal: 1m 48s\tremaining: 1m 13s\n",
            "417:\tlearn: 0.2391351\ttotal: 1m 48s\tremaining: 1m 13s\n",
            "418:\tlearn: 0.2388566\ttotal: 1m 48s\tremaining: 1m 13s\n",
            "419:\tlearn: 0.2385580\ttotal: 1m 49s\tremaining: 1m 12s\n",
            "420:\tlearn: 0.2379485\ttotal: 1m 49s\tremaining: 1m 12s\n",
            "421:\tlearn: 0.2375350\ttotal: 1m 49s\tremaining: 1m 12s\n",
            "422:\tlearn: 0.2371503\ttotal: 1m 49s\tremaining: 1m 11s\n",
            "423:\tlearn: 0.2369087\ttotal: 1m 50s\tremaining: 1m 11s\n",
            "424:\tlearn: 0.2367227\ttotal: 1m 50s\tremaining: 1m 11s\n",
            "425:\tlearn: 0.2365079\ttotal: 1m 50s\tremaining: 1m 11s\n",
            "426:\tlearn: 0.2363131\ttotal: 1m 50s\tremaining: 1m 10s\n",
            "427:\tlearn: 0.2357957\ttotal: 1m 51s\tremaining: 1m 10s\n",
            "428:\tlearn: 0.2353829\ttotal: 1m 51s\tremaining: 1m 10s\n",
            "429:\tlearn: 0.2351250\ttotal: 1m 51s\tremaining: 1m 10s\n",
            "430:\tlearn: 0.2346285\ttotal: 1m 52s\tremaining: 1m 9s\n",
            "431:\tlearn: 0.2343845\ttotal: 1m 52s\tremaining: 1m 9s\n",
            "432:\tlearn: 0.2340529\ttotal: 1m 52s\tremaining: 1m 9s\n",
            "433:\tlearn: 0.2337632\ttotal: 1m 52s\tremaining: 1m 9s\n",
            "434:\tlearn: 0.2333573\ttotal: 1m 53s\tremaining: 1m 8s\n",
            "435:\tlearn: 0.2328575\ttotal: 1m 53s\tremaining: 1m 8s\n",
            "436:\tlearn: 0.2326002\ttotal: 1m 53s\tremaining: 1m 8s\n",
            "437:\tlearn: 0.2323579\ttotal: 1m 53s\tremaining: 1m 8s\n",
            "438:\tlearn: 0.2321687\ttotal: 1m 54s\tremaining: 1m 7s\n",
            "439:\tlearn: 0.2319699\ttotal: 1m 54s\tremaining: 1m 7s\n",
            "440:\tlearn: 0.2318250\ttotal: 1m 54s\tremaining: 1m 7s\n",
            "441:\tlearn: 0.2314598\ttotal: 1m 54s\tremaining: 1m 7s\n",
            "442:\tlearn: 0.2311138\ttotal: 1m 55s\tremaining: 1m 6s\n",
            "443:\tlearn: 0.2308844\ttotal: 1m 55s\tremaining: 1m 6s\n",
            "444:\tlearn: 0.2304727\ttotal: 1m 55s\tremaining: 1m 6s\n",
            "445:\tlearn: 0.2301237\ttotal: 1m 55s\tremaining: 1m 6s\n",
            "446:\tlearn: 0.2299244\ttotal: 1m 56s\tremaining: 1m 5s\n",
            "447:\tlearn: 0.2297124\ttotal: 1m 56s\tremaining: 1m 5s\n",
            "448:\tlearn: 0.2295293\ttotal: 1m 56s\tremaining: 1m 5s\n",
            "449:\tlearn: 0.2291188\ttotal: 1m 57s\tremaining: 1m 5s\n",
            "450:\tlearn: 0.2287757\ttotal: 1m 57s\tremaining: 1m 4s\n",
            "451:\tlearn: 0.2283717\ttotal: 1m 57s\tremaining: 1m 4s\n",
            "452:\tlearn: 0.2280387\ttotal: 1m 57s\tremaining: 1m 4s\n",
            "453:\tlearn: 0.2276308\ttotal: 1m 58s\tremaining: 1m 4s\n",
            "454:\tlearn: 0.2273828\ttotal: 1m 58s\tremaining: 1m 3s\n",
            "455:\tlearn: 0.2272555\ttotal: 1m 58s\tremaining: 1m 3s\n",
            "456:\tlearn: 0.2269793\ttotal: 1m 58s\tremaining: 1m 3s\n",
            "457:\tlearn: 0.2266166\ttotal: 1m 59s\tremaining: 1m 2s\n",
            "458:\tlearn: 0.2262891\ttotal: 1m 59s\tremaining: 1m 2s\n",
            "459:\tlearn: 0.2259022\ttotal: 1m 59s\tremaining: 1m 2s\n",
            "460:\tlearn: 0.2255643\ttotal: 1m 59s\tremaining: 1m 2s\n",
            "461:\tlearn: 0.2253349\ttotal: 2m\tremaining: 1m 1s\n",
            "462:\tlearn: 0.2250030\ttotal: 2m\tremaining: 1m 1s\n",
            "463:\tlearn: 0.2245998\ttotal: 2m\tremaining: 1m 1s\n",
            "464:\tlearn: 0.2242604\ttotal: 2m 1s\tremaining: 1m 1s\n",
            "465:\tlearn: 0.2237990\ttotal: 2m 1s\tremaining: 1m\n",
            "466:\tlearn: 0.2233920\ttotal: 2m 1s\tremaining: 1m\n",
            "467:\tlearn: 0.2230902\ttotal: 2m 1s\tremaining: 1m\n",
            "468:\tlearn: 0.2228404\ttotal: 2m 2s\tremaining: 1m\n",
            "469:\tlearn: 0.2226803\ttotal: 2m 2s\tremaining: 59.9s\n",
            "470:\tlearn: 0.2223483\ttotal: 2m 2s\tremaining: 59.6s\n",
            "471:\tlearn: 0.2222250\ttotal: 2m 2s\tremaining: 59.4s\n",
            "472:\tlearn: 0.2219619\ttotal: 2m 3s\tremaining: 59.1s\n",
            "473:\tlearn: 0.2217948\ttotal: 2m 3s\tremaining: 58.8s\n",
            "474:\tlearn: 0.2216037\ttotal: 2m 3s\tremaining: 58.6s\n",
            "475:\tlearn: 0.2213635\ttotal: 2m 3s\tremaining: 58.3s\n",
            "476:\tlearn: 0.2210330\ttotal: 2m 4s\tremaining: 58s\n",
            "477:\tlearn: 0.2208797\ttotal: 2m 4s\tremaining: 57.8s\n",
            "478:\tlearn: 0.2206479\ttotal: 2m 4s\tremaining: 57.5s\n",
            "479:\tlearn: 0.2203348\ttotal: 2m 4s\tremaining: 57.3s\n",
            "480:\tlearn: 0.2202012\ttotal: 2m 5s\tremaining: 57s\n",
            "481:\tlearn: 0.2199512\ttotal: 2m 5s\tremaining: 56.8s\n",
            "482:\tlearn: 0.2197631\ttotal: 2m 5s\tremaining: 56.5s\n",
            "483:\tlearn: 0.2193135\ttotal: 2m 6s\tremaining: 56.2s\n",
            "484:\tlearn: 0.2189029\ttotal: 2m 6s\tremaining: 56s\n",
            "485:\tlearn: 0.2187933\ttotal: 2m 6s\tremaining: 55.7s\n",
            "486:\tlearn: 0.2185644\ttotal: 2m 6s\tremaining: 55.5s\n",
            "487:\tlearn: 0.2181786\ttotal: 2m 7s\tremaining: 55.2s\n",
            "488:\tlearn: 0.2179356\ttotal: 2m 7s\tremaining: 55s\n",
            "489:\tlearn: 0.2177857\ttotal: 2m 7s\tremaining: 54.7s\n",
            "490:\tlearn: 0.2173958\ttotal: 2m 7s\tremaining: 54.4s\n",
            "491:\tlearn: 0.2171451\ttotal: 2m 8s\tremaining: 54.2s\n",
            "492:\tlearn: 0.2169331\ttotal: 2m 8s\tremaining: 53.9s\n",
            "493:\tlearn: 0.2167986\ttotal: 2m 8s\tremaining: 53.7s\n",
            "494:\tlearn: 0.2165527\ttotal: 2m 8s\tremaining: 53.4s\n",
            "495:\tlearn: 0.2162281\ttotal: 2m 9s\tremaining: 53.1s\n",
            "496:\tlearn: 0.2161099\ttotal: 2m 9s\tremaining: 52.9s\n",
            "497:\tlearn: 0.2158858\ttotal: 2m 9s\tremaining: 52.6s\n",
            "498:\tlearn: 0.2155166\ttotal: 2m 9s\tremaining: 52.3s\n",
            "499:\tlearn: 0.2153046\ttotal: 2m 10s\tremaining: 52.1s\n",
            "500:\tlearn: 0.2149038\ttotal: 2m 10s\tremaining: 51.8s\n",
            "501:\tlearn: 0.2147113\ttotal: 2m 10s\tremaining: 51.5s\n",
            "502:\tlearn: 0.2145814\ttotal: 2m 10s\tremaining: 51.3s\n",
            "503:\tlearn: 0.2143481\ttotal: 2m 11s\tremaining: 51s\n",
            "504:\tlearn: 0.2143100\ttotal: 2m 11s\tremaining: 50.8s\n",
            "505:\tlearn: 0.2141283\ttotal: 2m 11s\tremaining: 50.5s\n",
            "506:\tlearn: 0.2137761\ttotal: 2m 12s\tremaining: 50.3s\n",
            "507:\tlearn: 0.2135387\ttotal: 2m 12s\tremaining: 50s\n",
            "508:\tlearn: 0.2133086\ttotal: 2m 12s\tremaining: 49.7s\n",
            "509:\tlearn: 0.2131021\ttotal: 2m 12s\tremaining: 49.5s\n",
            "510:\tlearn: 0.2128170\ttotal: 2m 13s\tremaining: 49.2s\n",
            "511:\tlearn: 0.2126168\ttotal: 2m 13s\tremaining: 48.9s\n",
            "512:\tlearn: 0.2122242\ttotal: 2m 13s\tremaining: 48.7s\n",
            "513:\tlearn: 0.2119586\ttotal: 2m 13s\tremaining: 48.4s\n",
            "514:\tlearn: 0.2117716\ttotal: 2m 14s\tremaining: 48.2s\n",
            "515:\tlearn: 0.2116941\ttotal: 2m 14s\tremaining: 47.9s\n",
            "516:\tlearn: 0.2115243\ttotal: 2m 14s\tremaining: 47.6s\n",
            "517:\tlearn: 0.2112424\ttotal: 2m 14s\tremaining: 47.4s\n",
            "518:\tlearn: 0.2107789\ttotal: 2m 15s\tremaining: 47.1s\n",
            "519:\tlearn: 0.2104640\ttotal: 2m 15s\tremaining: 46.9s\n",
            "520:\tlearn: 0.2101426\ttotal: 2m 15s\tremaining: 46.6s\n",
            "521:\tlearn: 0.2099999\ttotal: 2m 15s\tremaining: 46.3s\n",
            "522:\tlearn: 0.2098691\ttotal: 2m 16s\tremaining: 46.1s\n",
            "523:\tlearn: 0.2095058\ttotal: 2m 16s\tremaining: 45.8s\n",
            "524:\tlearn: 0.2092432\ttotal: 2m 16s\tremaining: 45.6s\n",
            "525:\tlearn: 0.2090103\ttotal: 2m 16s\tremaining: 45.3s\n",
            "526:\tlearn: 0.2088217\ttotal: 2m 17s\tremaining: 45s\n",
            "527:\tlearn: 0.2085406\ttotal: 2m 17s\tremaining: 44.8s\n",
            "528:\tlearn: 0.2083732\ttotal: 2m 17s\tremaining: 44.5s\n",
            "529:\tlearn: 0.2082974\ttotal: 2m 18s\tremaining: 44.3s\n",
            "530:\tlearn: 0.2079601\ttotal: 2m 18s\tremaining: 44s\n",
            "531:\tlearn: 0.2077661\ttotal: 2m 18s\tremaining: 43.7s\n",
            "532:\tlearn: 0.2074884\ttotal: 2m 18s\tremaining: 43.5s\n",
            "533:\tlearn: 0.2072110\ttotal: 2m 19s\tremaining: 43.2s\n",
            "534:\tlearn: 0.2067883\ttotal: 2m 19s\tremaining: 43s\n",
            "535:\tlearn: 0.2065601\ttotal: 2m 19s\tremaining: 42.7s\n",
            "536:\tlearn: 0.2063997\ttotal: 2m 19s\tremaining: 42.4s\n",
            "537:\tlearn: 0.2062576\ttotal: 2m 20s\tremaining: 42.2s\n",
            "538:\tlearn: 0.2060194\ttotal: 2m 20s\tremaining: 41.9s\n",
            "539:\tlearn: 0.2056649\ttotal: 2m 20s\tremaining: 41.7s\n",
            "540:\tlearn: 0.2052504\ttotal: 2m 20s\tremaining: 41.4s\n",
            "541:\tlearn: 0.2048947\ttotal: 2m 21s\tremaining: 41.1s\n",
            "542:\tlearn: 0.2045846\ttotal: 2m 21s\tremaining: 40.9s\n",
            "543:\tlearn: 0.2044024\ttotal: 2m 21s\tremaining: 40.6s\n",
            "544:\tlearn: 0.2041227\ttotal: 2m 21s\tremaining: 40.4s\n",
            "545:\tlearn: 0.2036680\ttotal: 2m 22s\tremaining: 40.1s\n",
            "546:\tlearn: 0.2033534\ttotal: 2m 22s\tremaining: 39.8s\n",
            "547:\tlearn: 0.2032150\ttotal: 2m 22s\tremaining: 39.6s\n",
            "548:\tlearn: 0.2029990\ttotal: 2m 22s\tremaining: 39.3s\n",
            "549:\tlearn: 0.2027135\ttotal: 2m 23s\tremaining: 39.1s\n",
            "550:\tlearn: 0.2025504\ttotal: 2m 23s\tremaining: 38.8s\n",
            "551:\tlearn: 0.2024561\ttotal: 2m 23s\tremaining: 38.6s\n",
            "552:\tlearn: 0.2020328\ttotal: 2m 24s\tremaining: 38.3s\n",
            "553:\tlearn: 0.2018911\ttotal: 2m 24s\tremaining: 38s\n",
            "554:\tlearn: 0.2017326\ttotal: 2m 24s\tremaining: 37.8s\n",
            "555:\tlearn: 0.2013748\ttotal: 2m 24s\tremaining: 37.5s\n",
            "556:\tlearn: 0.2011426\ttotal: 2m 25s\tremaining: 37.3s\n",
            "557:\tlearn: 0.2009515\ttotal: 2m 25s\tremaining: 37s\n",
            "558:\tlearn: 0.2008299\ttotal: 2m 25s\tremaining: 36.7s\n",
            "559:\tlearn: 0.2006036\ttotal: 2m 25s\tremaining: 36.5s\n",
            "560:\tlearn: 0.2003269\ttotal: 2m 26s\tremaining: 36.2s\n",
            "561:\tlearn: 0.2001767\ttotal: 2m 26s\tremaining: 36s\n",
            "562:\tlearn: 0.1998945\ttotal: 2m 26s\tremaining: 35.7s\n",
            "563:\tlearn: 0.1995797\ttotal: 2m 26s\tremaining: 35.4s\n",
            "564:\tlearn: 0.1994444\ttotal: 2m 27s\tremaining: 35.2s\n",
            "565:\tlearn: 0.1992051\ttotal: 2m 27s\tremaining: 34.9s\n",
            "566:\tlearn: 0.1990723\ttotal: 2m 27s\tremaining: 34.6s\n",
            "567:\tlearn: 0.1988500\ttotal: 2m 27s\tremaining: 34.4s\n",
            "568:\tlearn: 0.1986570\ttotal: 2m 28s\tremaining: 34.1s\n",
            "569:\tlearn: 0.1985447\ttotal: 2m 28s\tremaining: 33.9s\n",
            "570:\tlearn: 0.1983905\ttotal: 2m 28s\tremaining: 33.6s\n",
            "571:\tlearn: 0.1979822\ttotal: 2m 29s\tremaining: 33.3s\n",
            "572:\tlearn: 0.1977266\ttotal: 2m 29s\tremaining: 33.1s\n",
            "573:\tlearn: 0.1975897\ttotal: 2m 29s\tremaining: 32.8s\n",
            "574:\tlearn: 0.1972921\ttotal: 2m 29s\tremaining: 32.6s\n",
            "575:\tlearn: 0.1971171\ttotal: 2m 30s\tremaining: 32.3s\n",
            "576:\tlearn: 0.1969830\ttotal: 2m 30s\tremaining: 32s\n",
            "577:\tlearn: 0.1967094\ttotal: 2m 30s\tremaining: 31.8s\n",
            "578:\tlearn: 0.1965517\ttotal: 2m 30s\tremaining: 31.5s\n",
            "579:\tlearn: 0.1962864\ttotal: 2m 31s\tremaining: 31.3s\n",
            "580:\tlearn: 0.1959014\ttotal: 2m 31s\tremaining: 31s\n",
            "581:\tlearn: 0.1957728\ttotal: 2m 31s\tremaining: 30.7s\n",
            "582:\tlearn: 0.1955971\ttotal: 2m 31s\tremaining: 30.5s\n",
            "583:\tlearn: 0.1953196\ttotal: 2m 32s\tremaining: 30.2s\n",
            "584:\tlearn: 0.1951438\ttotal: 2m 32s\tremaining: 30s\n",
            "585:\tlearn: 0.1947659\ttotal: 2m 32s\tremaining: 29.7s\n",
            "586:\tlearn: 0.1945134\ttotal: 2m 32s\tremaining: 29.4s\n",
            "587:\tlearn: 0.1942311\ttotal: 2m 33s\tremaining: 29.2s\n",
            "588:\tlearn: 0.1938917\ttotal: 2m 33s\tremaining: 28.9s\n",
            "589:\tlearn: 0.1936942\ttotal: 2m 33s\tremaining: 28.7s\n",
            "590:\tlearn: 0.1935270\ttotal: 2m 33s\tremaining: 28.4s\n",
            "591:\tlearn: 0.1931633\ttotal: 2m 34s\tremaining: 28.1s\n",
            "592:\tlearn: 0.1927809\ttotal: 2m 34s\tremaining: 27.9s\n",
            "593:\tlearn: 0.1925186\ttotal: 2m 34s\tremaining: 27.6s\n",
            "594:\tlearn: 0.1923596\ttotal: 2m 35s\tremaining: 27.4s\n",
            "595:\tlearn: 0.1919461\ttotal: 2m 35s\tremaining: 27.1s\n",
            "596:\tlearn: 0.1918078\ttotal: 2m 35s\tremaining: 26.8s\n",
            "597:\tlearn: 0.1916627\ttotal: 2m 35s\tremaining: 26.6s\n",
            "598:\tlearn: 0.1914671\ttotal: 2m 36s\tremaining: 26.3s\n",
            "599:\tlearn: 0.1913003\ttotal: 2m 36s\tremaining: 26s\n",
            "600:\tlearn: 0.1910372\ttotal: 2m 36s\tremaining: 25.8s\n",
            "601:\tlearn: 0.1908667\ttotal: 2m 36s\tremaining: 25.5s\n",
            "602:\tlearn: 0.1905342\ttotal: 2m 37s\tremaining: 25.3s\n",
            "603:\tlearn: 0.1902299\ttotal: 2m 37s\tremaining: 25s\n",
            "604:\tlearn: 0.1899901\ttotal: 2m 37s\tremaining: 24.7s\n",
            "605:\tlearn: 0.1898373\ttotal: 2m 37s\tremaining: 24.5s\n",
            "606:\tlearn: 0.1897296\ttotal: 2m 38s\tremaining: 24.2s\n",
            "607:\tlearn: 0.1895748\ttotal: 2m 38s\tremaining: 24s\n",
            "608:\tlearn: 0.1895024\ttotal: 2m 38s\tremaining: 23.7s\n",
            "609:\tlearn: 0.1893087\ttotal: 2m 38s\tremaining: 23.4s\n",
            "610:\tlearn: 0.1889456\ttotal: 2m 39s\tremaining: 23.2s\n",
            "611:\tlearn: 0.1887524\ttotal: 2m 39s\tremaining: 22.9s\n",
            "612:\tlearn: 0.1884667\ttotal: 2m 39s\tremaining: 22.7s\n",
            "613:\tlearn: 0.1882382\ttotal: 2m 39s\tremaining: 22.4s\n",
            "614:\tlearn: 0.1878485\ttotal: 2m 40s\tremaining: 22.1s\n",
            "615:\tlearn: 0.1877315\ttotal: 2m 40s\tremaining: 21.9s\n",
            "616:\tlearn: 0.1874592\ttotal: 2m 40s\tremaining: 21.6s\n",
            "617:\tlearn: 0.1872425\ttotal: 2m 41s\tremaining: 21.4s\n",
            "618:\tlearn: 0.1871197\ttotal: 2m 41s\tremaining: 21.1s\n",
            "619:\tlearn: 0.1866407\ttotal: 2m 41s\tremaining: 20.8s\n",
            "620:\tlearn: 0.1865305\ttotal: 2m 41s\tremaining: 20.6s\n",
            "621:\tlearn: 0.1862541\ttotal: 2m 42s\tremaining: 20.3s\n",
            "622:\tlearn: 0.1860146\ttotal: 2m 42s\tremaining: 20.1s\n",
            "623:\tlearn: 0.1858780\ttotal: 2m 42s\tremaining: 19.8s\n",
            "624:\tlearn: 0.1856507\ttotal: 2m 42s\tremaining: 19.5s\n",
            "625:\tlearn: 0.1853124\ttotal: 2m 43s\tremaining: 19.3s\n",
            "626:\tlearn: 0.1851510\ttotal: 2m 43s\tremaining: 19s\n",
            "627:\tlearn: 0.1847665\ttotal: 2m 43s\tremaining: 18.8s\n",
            "628:\tlearn: 0.1845241\ttotal: 2m 43s\tremaining: 18.5s\n",
            "629:\tlearn: 0.1843734\ttotal: 2m 44s\tremaining: 18.2s\n",
            "630:\tlearn: 0.1840860\ttotal: 2m 44s\tremaining: 18s\n",
            "631:\tlearn: 0.1839093\ttotal: 2m 44s\tremaining: 17.7s\n",
            "632:\tlearn: 0.1836695\ttotal: 2m 44s\tremaining: 17.5s\n",
            "633:\tlearn: 0.1833976\ttotal: 2m 45s\tremaining: 17.2s\n",
            "634:\tlearn: 0.1832119\ttotal: 2m 45s\tremaining: 16.9s\n",
            "635:\tlearn: 0.1830260\ttotal: 2m 45s\tremaining: 16.7s\n",
            "636:\tlearn: 0.1828153\ttotal: 2m 45s\tremaining: 16.4s\n",
            "637:\tlearn: 0.1826876\ttotal: 2m 46s\tremaining: 16.1s\n",
            "638:\tlearn: 0.1824706\ttotal: 2m 46s\tremaining: 15.9s\n",
            "639:\tlearn: 0.1822962\ttotal: 2m 46s\tremaining: 15.6s\n",
            "640:\tlearn: 0.1819724\ttotal: 2m 46s\tremaining: 15.4s\n",
            "641:\tlearn: 0.1815996\ttotal: 2m 47s\tremaining: 15.1s\n",
            "642:\tlearn: 0.1812682\ttotal: 2m 47s\tremaining: 14.8s\n",
            "643:\tlearn: 0.1811097\ttotal: 2m 47s\tremaining: 14.6s\n",
            "644:\tlearn: 0.1809061\ttotal: 2m 48s\tremaining: 14.3s\n",
            "645:\tlearn: 0.1807871\ttotal: 2m 48s\tremaining: 14.1s\n",
            "646:\tlearn: 0.1806148\ttotal: 2m 48s\tremaining: 13.8s\n",
            "647:\tlearn: 0.1802836\ttotal: 2m 48s\tremaining: 13.5s\n",
            "648:\tlearn: 0.1800363\ttotal: 2m 49s\tremaining: 13.3s\n",
            "649:\tlearn: 0.1799180\ttotal: 2m 49s\tremaining: 13s\n",
            "650:\tlearn: 0.1797034\ttotal: 2m 49s\tremaining: 12.8s\n",
            "651:\tlearn: 0.1795070\ttotal: 2m 49s\tremaining: 12.5s\n",
            "652:\tlearn: 0.1793043\ttotal: 2m 50s\tremaining: 12.2s\n",
            "653:\tlearn: 0.1790670\ttotal: 2m 50s\tremaining: 12s\n",
            "654:\tlearn: 0.1787870\ttotal: 2m 50s\tremaining: 11.7s\n",
            "655:\tlearn: 0.1786055\ttotal: 2m 50s\tremaining: 11.5s\n",
            "656:\tlearn: 0.1783740\ttotal: 2m 51s\tremaining: 11.2s\n",
            "657:\tlearn: 0.1780544\ttotal: 2m 51s\tremaining: 10.9s\n",
            "658:\tlearn: 0.1778769\ttotal: 2m 51s\tremaining: 10.7s\n",
            "659:\tlearn: 0.1776798\ttotal: 2m 51s\tremaining: 10.4s\n",
            "660:\tlearn: 0.1773832\ttotal: 2m 52s\tremaining: 10.2s\n",
            "661:\tlearn: 0.1771684\ttotal: 2m 52s\tremaining: 9.9s\n",
            "662:\tlearn: 0.1769238\ttotal: 2m 52s\tremaining: 9.64s\n",
            "663:\tlearn: 0.1767733\ttotal: 2m 52s\tremaining: 9.38s\n",
            "664:\tlearn: 0.1766049\ttotal: 2m 53s\tremaining: 9.12s\n",
            "665:\tlearn: 0.1764675\ttotal: 2m 53s\tremaining: 8.86s\n",
            "666:\tlearn: 0.1763448\ttotal: 2m 53s\tremaining: 8.6s\n",
            "667:\tlearn: 0.1760734\ttotal: 2m 54s\tremaining: 8.34s\n",
            "668:\tlearn: 0.1758553\ttotal: 2m 54s\tremaining: 8.08s\n",
            "669:\tlearn: 0.1757696\ttotal: 2m 54s\tremaining: 7.82s\n",
            "670:\tlearn: 0.1756230\ttotal: 2m 54s\tremaining: 7.56s\n",
            "671:\tlearn: 0.1754316\ttotal: 2m 55s\tremaining: 7.3s\n",
            "672:\tlearn: 0.1752811\ttotal: 2m 55s\tremaining: 7.04s\n",
            "673:\tlearn: 0.1751069\ttotal: 2m 55s\tremaining: 6.78s\n",
            "674:\tlearn: 0.1749924\ttotal: 2m 55s\tremaining: 6.51s\n",
            "675:\tlearn: 0.1748502\ttotal: 2m 56s\tremaining: 6.25s\n",
            "676:\tlearn: 0.1746978\ttotal: 2m 56s\tremaining: 5.99s\n",
            "677:\tlearn: 0.1745376\ttotal: 2m 56s\tremaining: 5.73s\n",
            "678:\tlearn: 0.1742931\ttotal: 2m 56s\tremaining: 5.47s\n",
            "679:\tlearn: 0.1741147\ttotal: 2m 57s\tremaining: 5.21s\n",
            "680:\tlearn: 0.1739011\ttotal: 2m 57s\tremaining: 4.95s\n",
            "681:\tlearn: 0.1735504\ttotal: 2m 57s\tremaining: 4.69s\n",
            "682:\tlearn: 0.1733960\ttotal: 2m 58s\tremaining: 4.43s\n",
            "683:\tlearn: 0.1731934\ttotal: 2m 58s\tremaining: 4.17s\n",
            "684:\tlearn: 0.1730451\ttotal: 2m 58s\tremaining: 3.91s\n",
            "685:\tlearn: 0.1729622\ttotal: 2m 58s\tremaining: 3.65s\n",
            "686:\tlearn: 0.1727495\ttotal: 2m 59s\tremaining: 3.39s\n",
            "687:\tlearn: 0.1726661\ttotal: 2m 59s\tremaining: 3.13s\n",
            "688:\tlearn: 0.1725138\ttotal: 2m 59s\tremaining: 2.87s\n",
            "689:\tlearn: 0.1724568\ttotal: 2m 59s\tremaining: 2.61s\n",
            "690:\tlearn: 0.1723275\ttotal: 3m\tremaining: 2.35s\n",
            "691:\tlearn: 0.1720916\ttotal: 3m\tremaining: 2.09s\n",
            "692:\tlearn: 0.1716700\ttotal: 3m\tremaining: 1.82s\n",
            "693:\tlearn: 0.1715368\ttotal: 3m 1s\tremaining: 1.56s\n",
            "694:\tlearn: 0.1713164\ttotal: 3m 1s\tremaining: 1.3s\n",
            "695:\tlearn: 0.1711133\ttotal: 3m 1s\tremaining: 1.04s\n",
            "696:\tlearn: 0.1709301\ttotal: 3m 1s\tremaining: 783ms\n",
            "697:\tlearn: 0.1706637\ttotal: 3m 2s\tremaining: 522ms\n",
            "698:\tlearn: 0.1704770\ttotal: 3m 2s\tremaining: 261ms\n",
            "699:\tlearn: 0.1702635\ttotal: 3m 2s\tremaining: 0us\n",
            "Понадобилось времени на PCA + catboost с подбором числа деревьев и длины шага: 3425.599561214447\n",
            "Accuracy:  0.8621\n"
          ]
        }
      ],
      "source": [
        "# PCA -> CatBoost с подбором числа деревьев и длины шага\n",
        "\n",
        "beg = time.time()\n",
        "\n",
        "# PCA\n",
        "pca = PCA(n_components=50)\n",
        "x_train_pca = pca.fit_transform(x_train_loc)\n",
        "x_test_pca = pca.transform(x_test)\n",
        "\n",
        "# подбор числа деревьев и длины шага\n",
        "params = {'n_estimators': [10, 100, 500, 700], \n",
        "          'learning_rate': [0.01, 0.1, 0.5, 1]\n",
        "         }\n",
        "cb = GridSearchCV(CatBoostClassifier(silent=True), param_grid=params, cv=3, scoring='accuracy')\n",
        "cb.fit(x_train_pca, y_train_loc)\n",
        "\n",
        "# обучим лучший catboost \n",
        "best_params = cb.best_params_\n",
        "best_cb = CatBoostClassifier(**best_params)\n",
        "best_cb.fit(x_train_pca, y_train_loc)\n",
        "\n",
        "print('Понадобилось времени на PCA + catboost с подбором числа деревьев и длины шага:', time.time() - beg)\n",
        "print(\"Accuracy: \", accuracy_score(best_cb.predict(x_test_pca), y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Работает значительно дольше, чем подход со случайными признаками. Результат дает хороший, хотя мы ядровым SVM со случайными признаками можем получить результат лучше за всего 209 секунд. Поэтому такое большое время работы не оправдано, и подход со случайными признаками лучше.\n",
        "\n",
        "Получается, что подход со случайными признаками в целом оправданный: дает хорошее качество и работает за хорошее время (да, для линейного SVM время ожидания было в два раза больше, но не катастрофичное, можно подождать для хорошего результата. Или в одном случае мы получаем результат примерно такой же), можно брать в рассмотрение и пробовать!"
      ],
      "metadata": {
        "id": "ZGkIkkVBjQt-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6umjhWuK-hV"
      },
      "source": [
        "__Задание 3. (2 балла)__\n",
        "\n",
        "Проведите эксперименты:\n",
        "1. Помогает ли предварительное понижение размерности с помощью PCA? \n",
        "2. Как зависит итоговое качество от n_features? Выходит ли оно на плато при росте n_features?\n",
        "3. Важно ли, какую модель обучать — логистическую регрессию или SVM?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# поиск сигмы через цикл для озу\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class RFFPipeline(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, n_features=1000, new_dim=50, use_PCA=True, classifier='logreg'):\n",
        "        \"\"\"        \n",
        "        Implements pipeline, which consists of PCA decomposition,\n",
        "        Random Fourier Features approximation and linear classification model.\n",
        "        \n",
        "        n_features, int: amount of synthetic random features generated with RFF approximation.\n",
        "\n",
        "        new_dim, int: PCA output size.\n",
        "        \n",
        "        use_PCA, bool: whether to include PCA preprocessing.\n",
        "        \n",
        "        classifier, string: either 'svm' or 'logreg', a linear classification model to use on top of pipeline.\n",
        "        \n",
        "        Feel free to edit this template for your preferences.    \n",
        "        \"\"\"\n",
        "        self.n_features = n_features\n",
        "        self.new_dim = new_dim\n",
        "\n",
        "        self.use_PCA = use_PCA\n",
        "        self.PCA = PCA(n_components=self.new_dim)\n",
        "\n",
        "        self.classifier = classifier\n",
        "        if self.classifier == 'logreg':\n",
        "            self.model = LogisticRegression()\n",
        "        else:\n",
        "            self.model = SVC(kernel=self.classifier)\n",
        "  \n",
        "    def sigma_2(self, X):\n",
        "        # получим миллион пар (i_k, j_k)\n",
        "        i = np.random.choice(X.shape[0], size=1000000)\n",
        "        j = np.random.choice(X.shape[0], size=1000000)\n",
        "\n",
        "        # но некоторые могут быть вида (i, i), их нельзя сравнивать. просто выкинем\n",
        "        # т.к X достаточно большой, сильно на числе пар это не отразится\n",
        "        i, j = i[i!=j], j[i!=j]\n",
        "\n",
        "        # результат\n",
        "        return np.median(np.sum((X[i] - X[j])**2, axis=1))\n",
        "\n",
        "    def sigma_2_for(self, X):\n",
        "      sigmas = []\n",
        "      for _ in range(1000000):\n",
        "        i, j = np.random.randint(0, X.shape[0], size=2)\n",
        "        if i != j:\n",
        "          sigmas.append(np.linalg.norm(X[i] - X[j])**2)\n",
        "      return np.median(sigmas)\n",
        "\n",
        "    def use_PCA_func(self, X):\n",
        "        if self.use_PCA:\n",
        "            X = self.PCA.transform(X)\n",
        "        else:\n",
        "            self.new_dim = X.shape[1]\n",
        "        return X\n",
        "\n",
        "    def new_feat(self, X):\n",
        "      return np.cos(X.dot(self.w) + self.b)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit all parts of algorithm (PCA, RFF, Classification) to training set.\n",
        "        \"\"\"\n",
        "        self.PCA.fit(X)\n",
        "        X = self.use_PCA_func(X)\n",
        "\n",
        "        self.sigma = self.sigma_2_for(X)\n",
        "        self.w = np.random.normal(0, 1/np.sqrt(self.sigma), size=(self.new_dim, self.n_features))\n",
        "        self.b = np.random.uniform(-np.pi, np.pi, size=self.n_features)\n",
        "\n",
        "        new_X = self.new_feat(X)\n",
        "        self.model.fit(new_X, y)\n",
        "        \n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Apply pipeline to obtain scores for input data.\n",
        "        \"\"\"\n",
        "        X = self.use_PCA_func(X)\n",
        "        new_X = self.new_feat(X)\n",
        "        return self.model.predict_proba(new_X) \n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Apply pipeline to obtain discrete predictions for input data.\n",
        "        \"\"\"\n",
        "        X = self.use_PCA_func(X)\n",
        "        new_X = self.new_feat(X)\n",
        "        return self.model.predict(new_X)"
      ],
      "metadata": {
        "id": "soOLr6f7P9V-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56c4ed01-b7c9-4efb-d68d-19a20ff9de81",
        "id": "0ZDwZTEAsqeH"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Времени потребовалось на kernel SVM без PCA: 209.0069181919098\n",
            "Accuracy: 0.8668\n",
            "Времени потребовалось на kernel SVM с PCA: 189.69252824783325\n",
            "Accuracy: 0.8547\n"
          ]
        }
      ],
      "source": [
        "# 1\n",
        "\n",
        "# kernel SVM\n",
        "\n",
        "beg = time.time()\n",
        "\n",
        "without_pca = RFFPipeline(classifier='rbf', use_PCA=False)\n",
        "without_pca.fit(x_train_loc, y_train_loc)\n",
        "\n",
        "print(\"Времени потребовалось на kernel SVM без PCA:\", time.time() - beg)\n",
        "print(\"Accuracy:\", accuracy_score(without_pca.predict(x_test), y_test))\n",
        "\n",
        "beg = time.time()\n",
        "\n",
        "with_pca = RFFPipeline(classifier='rbf', use_PCA=True)\n",
        "with_pca.fit(x_train_loc, y_train_loc)\n",
        "\n",
        "print(\"Времени потребовалось на kernel SVM с PCA:\", time.time() - beg)\n",
        "print(\"Accuracy:\", accuracy_score(with_pca.predict(x_test), y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA чуть ускорило выполнение и чуть-чуть испортило качество. Не похоже, что использовать или нет особо имеет значение."
      ],
      "metadata": {
        "id": "bK-pzSTctCz9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f539045e-8b36-4097-8204-71061e9f6323",
        "id": "ePCvd6jLuxAU"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Времени потребовалось на SVM без PCA: 196.2081913948059\n",
            "Accuracy: 0.8397\n",
            "Времени потребовалось на SVM c PCA: 176.56715750694275\n",
            "Accuracy: 0.8675\n"
          ]
        }
      ],
      "source": [
        "# linear SVM\n",
        "\n",
        "beg = time.time()\n",
        "\n",
        "without_pca = RFFPipeline(classifier='linear', use_PCA=False)\n",
        "without_pca.fit(x_train_loc, y_train_loc)\n",
        "\n",
        "print(\"Времени потребовалось на SVM без PCA:\", time.time() - beg)\n",
        "print(\"Accuracy:\", accuracy_score(without_pca.predict(x_test), y_test))\n",
        "\n",
        "beg = time.time()\n",
        "\n",
        "with_pca = RFFPipeline(classifier='linear', use_PCA=True)\n",
        "with_pca.fit(x_train_loc, y_train_loc)\n",
        "\n",
        "print(\"Времени потребовалось на SVM c PCA:\", time.time() - beg)\n",
        "print(\"Accuracy:\", accuracy_score(with_pca.predict(x_test), y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA ускорило работу алгоритма и улучшило качество. Использование однозначно оправдано."
      ],
      "metadata": {
        "id": "Kep7ZTKEu1-X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94a20280-487d-4496-e0c6-8d12eb526135",
        "id": "eX9R293jvS-t"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Времени потребовалось на logreg без PCA: 68.27404165267944\n",
            "Accuracy: 0.8501\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Времени потребовалось на logreg c PCA: 66.47967100143433\n",
            "Accuracy: 0.8554\n"
          ]
        }
      ],
      "source": [
        "# logreg\n",
        "beg = time.time()\n",
        "\n",
        "without_pca = RFFPipeline(classifier='logreg', use_PCA=False)\n",
        "without_pca.fit(x_train_loc, y_train_loc)\n",
        "\n",
        "print(\"Времени потребовалось на logreg без PCA:\", time.time() - beg)\n",
        "print(\"Accuracy:\", accuracy_score(without_pca.predict(x_test), y_test))\n",
        "\n",
        "beg = time.time()\n",
        "\n",
        "with_pca = RFFPipeline(classifier='logreg', use_PCA=True)\n",
        "with_pca.fit(x_train_loc, y_train_loc)\n",
        "\n",
        "print(\"Времени потребовалось на logreg c PCA:\", time.time() - beg)\n",
        "print(\"Accuracy:\", accuracy_score(with_pca.predict(x_test), y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(извините, что не отключила ворнинги))\n",
        "\n",
        "PCA чуть ускорило работу и чуть улучшило качество. Изменения некритичные, конечно, но и не  худшую сторону))\n",
        "\n",
        "Общий вывод: использование оправдано, ведь PCA или хорошо улучшает качество и время работы, или почти не меняет эти показатели."
      ],
      "metadata": {
        "id": "fJGARqwivU2M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56dc8dff-e2bf-4d49-9e3b-f8531554d715",
        "id": "UHAL1iZUaJn8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        }
      ],
      "source": [
        "# 2\n",
        "\n",
        "# logreg\n",
        "\n",
        "acc_logreg = []\n",
        "n_feat = np.linspace(10, 3000, 50, dtype=int)\n",
        "\n",
        "for feat in n_feat:\n",
        "    model = RFFPipeline(classifier='logreg', n_features=feat)\n",
        "    model.fit(x_train_loc, y_train_loc)\n",
        "    acc_logreg.append(accuracy_score(model.predict(x_test), y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        },
        "outputId": "3074385b-daaf-417c-cb7d-c71b8d7fed3b",
        "id": "Zx4nNRU0aQg4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'accuracy')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# accuracy от n_features\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.title('График зависимости accuracy от n_features для logreg')\n",
        "plt.plot(np.linspace(10, 3000, 50, dtype=int), acc_logreg)\n",
        "plt.xlabel('n_features')\n",
        "plt.ylabel('accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Видим, что с ростом n_features где-то до 350 качество растет, а потом выходит на плато."
      ],
      "metadata": {
        "id": "A05J2A2KjE_B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gUKYhmuerHH",
        "outputId": "fc33bc64-57cd-4e7e-e8f5-ea04bcae0eed"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        }
      ],
      "source": [
        "# linear SVM\n",
        "\n",
        "acc_linSVM = []\n",
        "n_feat = np.linspace(10, 3000, 50, dtype=int)\n",
        "\n",
        "for feat in n_feat:\n",
        "    model = RFFPipeline(classifier='linear', n_features=feat)\n",
        "    model.fit(x_train_loc, y_train_loc)\n",
        "    acc_linSVM.append(accuracy_score(model.predict(x_test), y_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16, 8))\n",
        "plt.title('График зависимости accuracy от n_features для linear SVM')\n",
        "plt.plot(np.linspace(10, 3000, 50, dtype=int), acc_linSVM)\n",
        "plt.xlabel('n_features')\n",
        "plt.ylabel('accuracy')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "PeNP8LgDhJO8",
        "outputId": "a03aa40c-4248-4bdf-8093-452d9f313e58"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text(0, 0.5, 'accuracy')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Точно то же: качество поднимается где-то до 350, потом выходт на плато."
      ],
      "metadata": {
        "id": "O0EdclIkjYy7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3anovruELvs"
      },
      "source": [
        "\\#3\n",
        "\n",
        "Регрессия работает значительно быстрее, но при SVM качество чуть лучше (не соизмеримо с увеличением длительности работы). Таким образом, разница есть, если нам важно время работы. Если нет, то скорее нет."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJqXVuasK-hW"
      },
      "source": [
        "### Бонус"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVDWHCdrK-hX"
      },
      "source": [
        "__Задание 4. (Максимум 2 балла)__\n",
        "\n",
        "Как вы, должно быть, помните с курса МО-1, многие алгоритмы машинного обучения работают лучше, если признаки данных некоррелированы. Оказывается, что для RFF существует модификация, позволяющая получать ортогональные случайные признаки (Orthogonal Random Features, ORF). Об этом методе можно прочитать в [статье](https://proceedings.neurips.cc/paper/2016/file/53adaf494dc89ef7196d73636eb2451b-Paper.pdf). Реализуйте класс для вычисления ORF по аналогии с основным заданием. Обратите внимание, что ваш класс должен уметь работать со случаем n_features > new_dim (в статье есть замечание на этот счет). Проведите эксперименты, сравнивающие RFF и ORF, сделайте выводы."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTjFFIjMDotE"
      },
      "source": [
        "$$W_{ORF} = \\frac{1}{\\sigma} SQ$$\n",
        "\n",
        "Q --- равномерно распределенная ортогональная матрица из QR-разложения матрицы Гаусса. S --- диагональная с $\\chi$-распределением на диагонали.\n",
        "\n",
        "Замечание: если n_features < new_dim, то используем первые new_dim D столбцов. Иначе мы используем несколько независимо сгенерированных случайных объектов и объединяем результаты."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSxvGI9iK-hX"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class ORFPipeline(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, n_features=1000, new_dim=50, use_PCA=True, classifier='logreg'):\n",
        "        \"\"\"        \n",
        "        Implements pipeline, which consists of PCA decomposition,\n",
        "        Random Fourier Features approximation and linear classification model.\n",
        "        \n",
        "        n_features, int: amount of synthetic random features generated with RFF approximation.\n",
        "\n",
        "        new_dim, int: PCA output size.\n",
        "        \n",
        "        use_PCA, bool: whether to include PCA preprocessing.\n",
        "        \n",
        "        classifier, string: either 'svm' or 'logreg', a linear classification model to use on top of pipeline.\n",
        "        \n",
        "        Feel free to edit this template for your preferences.    \n",
        "        \"\"\"\n",
        "        self.n_features = n_features\n",
        "        self.new_dim = new_dim\n",
        "\n",
        "        self.use_PCA = use_PCA\n",
        "        self.PCA = PCA(n_components=self.new_dim)\n",
        "\n",
        "        self.classifier = classifier\n",
        "        if self.classifier == 'logreg':\n",
        "            self.model = LogisticRegression()\n",
        "        else:\n",
        "            self.model = SVC(kernel=self.classifier)\n",
        "  \n",
        "    def sigma_2(self, X):\n",
        "        # получим миллион пар (i_k, j_k)\n",
        "        i = np.random.choice(X.shape[0], size=1000000)\n",
        "        j = np.random.choice(X.shape[0], size=1000000)\n",
        "\n",
        "        # но некоторые могут быть вида (i, i), их нельзя сравнивать. просто выкинем\n",
        "        # т.к X достаточно большой, сильно на числе пар это не отразится\n",
        "        i, j = i[i!=j], j[i!=j]\n",
        "\n",
        "        # результат\n",
        "        return np.median(np.sum((X[i] - X[j])**2, axis=1))\n",
        "\n",
        "    def use_PCA_func(self, X):\n",
        "        if self.use_PCA:\n",
        "            X = self.PCA.transform(X)\n",
        "        else:\n",
        "            self.new_dim = X.shape[1]\n",
        "        return X\n",
        "\n",
        "    def new_feat(self, X):\n",
        "      return np.cos(X.dot(self.w.T) + self.b)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit all parts of algorithm (PCA, RFF, Classification) to training set.\n",
        "        \"\"\"\n",
        "        self.PCA.fit(X)\n",
        "        X = self.use_PCA_func(X)\n",
        "\n",
        "        self.sigma = self.sigma_2(X)\n",
        "\n",
        "        G = np.random.normal(size=(self.n_features, self.new_dim))\n",
        "        S, _ = np.linalg.qr(G)\n",
        "        Q = np.diag(np.sqrt(np.random.chisquare(self.new_dim, self.new_dim)))\n",
        "\n",
        "        self.w = 1 / np.sqrt(self.sigma) * np.dot(S, Q)\n",
        "        self.b = np.random.uniform(-np.pi, np.pi, size=self.n_features)\n",
        "\n",
        "        new_X = self.new_feat(X)\n",
        "        self.model.fit(new_X, y)\n",
        "        \n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Apply pipeline to obtain scores for input data.\n",
        "        \"\"\"\n",
        "        X = self.use_PCA_func(X)\n",
        "        new_X = self.new_feat(X)\n",
        "        return self.model.predict_proba(new_X) \n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Apply pipeline to obtain discrete predictions for input data.\n",
        "        \"\"\"\n",
        "        X = self.use_PCA_func(X)\n",
        "        new_X = self.new_feat(X)\n",
        "        return self.model.predict(new_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a34d274-080e-47b8-fe89-87f38cd5c594",
        "id": "iIjF9uJ9aA7-"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Времени потребовалось: 47.03280067443848\n",
            "Accuracy: 0.8048\n"
          ]
        }
      ],
      "source": [
        "beg = time.time()\n",
        "\n",
        "ORF_logreg = ORFPipeline(classifier='logreg')\n",
        "ORF_logreg.fit(x_train_loc, y_train_loc)\n",
        "\n",
        "print(\"Времени потребовалось:\", time.time() - beg)\n",
        "print(\"Accuracy:\", accuracy_score(ORF_logreg.predict(x_test), y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "напомним результаты RFF с logreg: работало за 66.48 сек, качество 0.86. Получается, ORF ухудшило качество, но ускорило работу."
      ],
      "metadata": {
        "id": "3Xk-z8aywEIE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c56c8d73-57a0-4f9e-82c6-56519167b5e1",
        "id": "fI4XrNsulDnY"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Времени потребовалось: 166.7388105392456\n",
            "Accuracy: 0.8494\n"
          ]
        }
      ],
      "source": [
        "beg = time.time()\n",
        "\n",
        "ORF_linear = ORFPipeline(classifier='linear', use_PCA=True)\n",
        "ORF_linear.fit(x_train_loc, y_train_loc)\n",
        "\n",
        "print(\"Времени потребовалось:\", time.time() - beg)\n",
        "print(\"Accuracy:\", accuracy_score(ORF_linear.predict(x_test), y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Напомним результаты RFF с линейным SVM: работает за 176.6 сек с качеством 0.87. Т.е. качество ухудшилось, а время чуть уменьшилось."
      ],
      "metadata": {
        "id": "zegLEgdy8mhG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f392a80-0f54-49b3-c089-67c7031d8f53",
        "id": "eCG5KnR5aIr4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Времени потребовалось: 308.3632354736328\n",
            "Accuracy: 0.7986\n"
          ]
        }
      ],
      "source": [
        "beg = time.time()\n",
        "\n",
        "ORF_rbf = ORFPipeline(classifier='rbf', use_PCA=True)\n",
        "ORF_rbf.fit(x_train_loc, y_train_loc)\n",
        "\n",
        "print(\"Времени потребовалось:\", time.time() - beg)\n",
        "print(\"Accuracy:\", accuracy_score(ORF_rbf.predict(x_test), y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Напомним результаты RFF с ядровым SVM: работает за 189.7 сек с качеством 0.85. RFF работает и значительно быстрее, и с лучшим качеством. Тут совсем провал: и дольше, и качество хуже.\n",
        "\n",
        "Общий вывод: тут не подошло. Нам лучше подходит RFF."
      ],
      "metadata": {
        "id": "0_Vl4l4e8vWu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pc7-1jmK-hY"
      },
      "source": [
        "__Задание 5. (Максимум 2 балла)__\n",
        "\n",
        "Поэкспериментируйте с функциями для вычисления новых случайных признаков. Не обязательно использовать косинус от скалярного произведения — можно брать знак от него, хэш и т.д. Придумайте побольше вариантов для генерации признаков и проверьте, не получается ли с их помощью добиваться более высокого качества. Также можете попробовать другой классификатор поверх случайных признаков, сравните результаты."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtQz4J5gUTTF"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWj-O2vjK-hY"
      },
      "outputs": [],
      "source": [
        "# знак\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class RFFPipeline(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, n_features=1000, new_dim=50, use_PCA=True, classifier='logreg'):\n",
        "        \"\"\"        \n",
        "        Implements pipeline, which consists of PCA decomposition,\n",
        "        Random Fourier Features approximation and linear classification model.\n",
        "        \n",
        "        n_features, int: amount of synthetic random features generated with RFF approximation.\n",
        "\n",
        "        new_dim, int: PCA output size.\n",
        "        \n",
        "        use_PCA, bool: whether to include PCA preprocessing.\n",
        "        \n",
        "        classifier, string: either 'svm' or 'logreg', a linear classification model to use on top of pipeline.\n",
        "        \n",
        "        Feel free to edit this template for your preferences.    \n",
        "        \"\"\"\n",
        "        self.n_features = n_features\n",
        "        self.new_dim = new_dim\n",
        "\n",
        "        self.use_PCA = use_PCA\n",
        "        self.PCA = PCA(n_components=self.new_dim)\n",
        "\n",
        "        self.classifier = classifier\n",
        "        if self.classifier == 'logreg':\n",
        "            self.model = LogisticRegression()\n",
        "        else:\n",
        "            self.model = SVC(kernel=self.classifier)\n",
        "  \n",
        "    def sigma_2(self, X):\n",
        "        # получим миллион пар (i_k, j_k)\n",
        "        i = np.random.choice(X.shape[0], size=1000000)\n",
        "        j = np.random.choice(X.shape[0], size=1000000)\n",
        "\n",
        "        # но некоторые могут быть вида (i, i), их нельзя сравнивать. просто выкинем\n",
        "        # т.к X достаточно большой, сильно на числе пар это не отразится\n",
        "        i, j = i[i!=j], j[i!=j]\n",
        "\n",
        "        # результат\n",
        "        return np.median(np.sum((X[i] - X[j])**2, axis=1))\n",
        "\n",
        "    def use_PCA_func(self, X):\n",
        "        if self.use_PCA:\n",
        "            X = self.PCA.transform(X)\n",
        "        else:\n",
        "            self.new_dim = X.shape[1]\n",
        "        return X\n",
        "\n",
        "    def new_feat(self, X):\n",
        "      return np.sign(X.dot(self.w) + self.b)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit all parts of algorithm (PCA, RFF, Classification) to training set.\n",
        "        \"\"\"\n",
        "        self.PCA.fit(X)\n",
        "        X = self.use_PCA_func(X)\n",
        "\n",
        "        self.sigma = self.sigma_2(X)\n",
        "        self.w = np.random.normal(0, 1/np.sqrt(self.sigma), size=(self.new_dim, self.n_features))\n",
        "        self.b = np.random.uniform(-np.pi, np.pi, size=self.n_features)\n",
        "\n",
        "        new_X = self.new_feat(X)\n",
        "        self.model.fit(new_X, y)\n",
        "        \n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Apply pipeline to obtain scores for input data.\n",
        "        \"\"\"\n",
        "        X = self.use_PCA_func(X)\n",
        "        new_X = self.new_feat(X)\n",
        "        return self.model.predict_proba(new_X) \n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Apply pipeline to obtain discrete predictions for input data.\n",
        "        \"\"\"\n",
        "        X = self.use_PCA_func(X)\n",
        "        new_X = self.new_feat(X)\n",
        "        return self.model.predict(new_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62cdcbe7-0910-4a20-d8f8-8980132f4cdf",
        "id": "_3uFuj1kekxj"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Выполняется за 42.32986354827881\n",
            "Accuracy: 0.8216\n"
          ]
        }
      ],
      "source": [
        "beg = time.time()\n",
        "\n",
        "sign_model = RFFPipeline()\n",
        "sign_model.fit(x_train_loc, y_train_loc)\n",
        "\n",
        "print(\"Выполняется за\", time.time() - beg)\n",
        "print(\"Accuracy:\", accuracy_score(sign_model.predict(x_test), y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43AK4VRGRYDx"
      },
      "outputs": [],
      "source": [
        "# exp\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class RFFPipeline(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, n_features=1000, new_dim=50, use_PCA=True, classifier='logreg'):\n",
        "        \"\"\"        \n",
        "        Implements pipeline, which consists of PCA decomposition,\n",
        "        Random Fourier Features approximation and linear classification model.\n",
        "        \n",
        "        n_features, int: amount of synthetic random features generated with RFF approximation.\n",
        "\n",
        "        new_dim, int: PCA output size.\n",
        "        \n",
        "        use_PCA, bool: whether to include PCA preprocessing.\n",
        "        \n",
        "        classifier, string: either 'svm' or 'logreg', a linear classification model to use on top of pipeline.\n",
        "        \n",
        "        Feel free to edit this template for your preferences.    \n",
        "        \"\"\"\n",
        "        self.n_features = n_features\n",
        "        self.new_dim = new_dim\n",
        "\n",
        "        self.use_PCA = use_PCA\n",
        "        self.PCA = PCA(n_components=self.new_dim)\n",
        "\n",
        "        self.classifier = classifier\n",
        "        if self.classifier == 'logreg':\n",
        "            self.model = LogisticRegression()\n",
        "        else:\n",
        "            self.model = SVC(kernel=self.classifier)\n",
        "  \n",
        "    def sigma_2(self, X):\n",
        "        # получим миллион пар (i_k, j_k)\n",
        "        i = np.random.choice(X.shape[0], size=1000000)\n",
        "        j = np.random.choice(X.shape[0], size=1000000)\n",
        "\n",
        "        # но некоторые могут быть вида (i, i), их нельзя сравнивать. просто выкинем\n",
        "        # т.к X достаточно большой, сильно на числе пар это не отразится\n",
        "        i, j = i[i!=j], j[i!=j]\n",
        "\n",
        "        # результат\n",
        "        return np.median(np.sum((X[i] - X[j])**2, axis=1))\n",
        "\n",
        "    def use_PCA_func(self, X):\n",
        "        if self.use_PCA:\n",
        "            X = self.PCA.transform(X)\n",
        "        else:\n",
        "            self.new_dim = X.shape[1]\n",
        "        return X\n",
        "\n",
        "    def new_feat(self, X):\n",
        "      return np.exp(X.dot(self.w) + self.b)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit all parts of algorithm (PCA, RFF, Classification) to training set.\n",
        "        \"\"\"\n",
        "        self.PCA.fit(X)\n",
        "        X = self.use_PCA_func(X)\n",
        "\n",
        "        self.sigma = self.sigma_2(X)\n",
        "        self.w = np.random.normal(0, 1/np.sqrt(self.sigma), size=(self.new_dim, self.n_features))\n",
        "        self.b = np.random.uniform(-np.pi, np.pi, size=self.n_features)\n",
        "\n",
        "        new_X = self.new_feat(X)\n",
        "        self.model.fit(new_X, y)\n",
        "        \n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Apply pipeline to obtain scores for input data.\n",
        "        \"\"\"\n",
        "        X = self.use_PCA_func(X)\n",
        "        new_X = self.new_feat(X)\n",
        "        return self.model.predict_proba(new_X) \n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Apply pipeline to obtain discrete predictions for input data.\n",
        "        \"\"\"\n",
        "        X = self.use_PCA_func(X)\n",
        "        new_X = self.new_feat(X)\n",
        "        return self.model.predict(new_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fa4f39a-a089-4dfc-a041-0962594d2813",
        "id": "KohI3ThFe-XV"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Выполняется за 64.60627508163452\n",
            "Accuracy: 0.8113\n"
          ]
        }
      ],
      "source": [
        "beg = time.time()\n",
        "\n",
        "exp_model = RFFPipeline()\n",
        "exp_model.fit(x_train_loc, y_train_loc)\n",
        "\n",
        "print(\"Выполняется за\", time.time() - beg)\n",
        "print(\"Accuracy:\", accuracy_score(exp_model.predict(x_test), y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUBcJt1cRtYR"
      },
      "outputs": [],
      "source": [
        "# log\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class RFFPipeline(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, n_features=1000, new_dim=50, use_PCA=True, classifier='logreg'):\n",
        "        \"\"\"        \n",
        "        Implements pipeline, which consists of PCA decomposition,\n",
        "        Random Fourier Features approximation and linear classification model.\n",
        "        \n",
        "        n_features, int: amount of synthetic random features generated with RFF approximation.\n",
        "\n",
        "        new_dim, int: PCA output size.\n",
        "        \n",
        "        use_PCA, bool: whether to include PCA preprocessing.\n",
        "        \n",
        "        classifier, string: either 'svm' or 'logreg', a linear classification model to use on top of pipeline.\n",
        "        \n",
        "        Feel free to edit this template for your preferences.    \n",
        "        \"\"\"\n",
        "        self.n_features = n_features\n",
        "        self.new_dim = new_dim\n",
        "\n",
        "        self.use_PCA = use_PCA\n",
        "        self.PCA = PCA(n_components=self.new_dim)\n",
        "\n",
        "        self.classifier = classifier\n",
        "        if self.classifier == 'logreg':\n",
        "            self.model = LogisticRegression()\n",
        "        else:\n",
        "            self.model = SVC(kernel=self.classifier)\n",
        "  \n",
        "    def sigma_2(self, X):\n",
        "        # получим миллион пар (i_k, j_k)\n",
        "        i = np.random.choice(X.shape[0], size=1000000)\n",
        "        j = np.random.choice(X.shape[0], size=1000000)\n",
        "\n",
        "        # но некоторые могут быть вида (i, i), их нельзя сравнивать. просто выкинем\n",
        "        # т.к X достаточно большой, сильно на числе пар это не отразится\n",
        "        i, j = i[i!=j], j[i!=j]\n",
        "\n",
        "        # результат\n",
        "        return np.median(np.sum((X[i] - X[j])**2, axis=1))\n",
        "\n",
        "    def use_PCA_func(self, X):\n",
        "        if self.use_PCA:\n",
        "            X = self.PCA.transform(X)\n",
        "        else:\n",
        "            self.new_dim = X.shape[1]\n",
        "        return X\n",
        "\n",
        "    def new_feat(self, X):\n",
        "      return np.log(np.abs(X.dot(self.w) + self.b))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit all parts of algorithm (PCA, RFF, Classification) to training set.\n",
        "        \"\"\"\n",
        "        self.PCA.fit(X)\n",
        "        X = self.use_PCA_func(X)\n",
        "\n",
        "        self.sigma = self.sigma_2(X)\n",
        "        self.w = np.random.normal(0, 1/np.sqrt(self.sigma), size=(self.new_dim, self.n_features))\n",
        "        self.b = np.random.uniform(-np.pi, np.pi, size=self.n_features)\n",
        "\n",
        "        new_X = self.new_feat(X)\n",
        "        self.model.fit(new_X, y)\n",
        "        \n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Apply pipeline to obtain scores for input data.\n",
        "        \"\"\"\n",
        "        X = self.use_PCA_func(X)\n",
        "        new_X = self.new_feat(X)\n",
        "        return self.model.predict_proba(new_X) \n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Apply pipeline to obtain discrete predictions for input data.\n",
        "        \"\"\"\n",
        "        X = self.use_PCA_func(X)\n",
        "        new_X = self.new_feat(X)\n",
        "        return self.model.predict(new_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c85c537-e744-4f40-b158-a6d68f44ea1d",
        "id": "-g_mYLVdfEUI"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Выполняется за 43.64604830741882\n",
            "Accuracy: 0.8309\n"
          ]
        }
      ],
      "source": [
        "beg = time.time()\n",
        "\n",
        "log_model = RFFPipeline()\n",
        "log_model.fit(x_train_loc, y_train_loc)\n",
        "\n",
        "print(\"Выполняется за\", time.time() - beg)\n",
        "print(\"Accuracy:\", accuracy_score(log_model.predict(x_test), y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOaIS1vqR11s"
      },
      "outputs": [],
      "source": [
        "# sin(exp)\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class RFFPipeline(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, n_features=1000, new_dim=50, use_PCA=True, classifier='logreg'):\n",
        "        \"\"\"        \n",
        "        Implements pipeline, which consists of PCA decomposition,\n",
        "        Random Fourier Features approximation and linear classification model.\n",
        "        \n",
        "        n_features, int: amount of synthetic random features generated with RFF approximation.\n",
        "\n",
        "        new_dim, int: PCA output size.\n",
        "        \n",
        "        use_PCA, bool: whether to include PCA preprocessing.\n",
        "        \n",
        "        classifier, string: either 'svm' or 'logreg', a linear classification model to use on top of pipeline.\n",
        "        \n",
        "        Feel free to edit this template for your preferences.    \n",
        "        \"\"\"\n",
        "        self.n_features = n_features\n",
        "        self.new_dim = new_dim\n",
        "\n",
        "        self.use_PCA = use_PCA\n",
        "        self.PCA = PCA(n_components=self.new_dim)\n",
        "\n",
        "        self.classifier = classifier\n",
        "        if self.classifier == 'logreg':\n",
        "            self.model = LogisticRegression()\n",
        "        else:\n",
        "            self.model = SVC(kernel=self.classifier)\n",
        "  \n",
        "    def sigma_2(self, X):\n",
        "        # получим миллион пар (i_k, j_k)\n",
        "        i = np.random.choice(X.shape[0], size=1000000)\n",
        "        j = np.random.choice(X.shape[0], size=1000000)\n",
        "\n",
        "        # но некоторые могут быть вида (i, i), их нельзя сравнивать. просто выкинем\n",
        "        # т.к X достаточно большой, сильно на числе пар это не отразится\n",
        "        i, j = i[i!=j], j[i!=j]\n",
        "\n",
        "        # результат\n",
        "        return np.median(np.sum((X[i] - X[j])**2, axis=1))\n",
        "\n",
        "    def use_PCA_func(self, X):\n",
        "        if self.use_PCA:\n",
        "            X = self.PCA.transform(X)\n",
        "        else:\n",
        "            self.new_dim = X.shape[1]\n",
        "        return X\n",
        "\n",
        "    def new_feat(self, X):\n",
        "      return np.sin(np.exp(X.dot(self.w) + self.b))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit all parts of algorithm (PCA, RFF, Classification) to training set.\n",
        "        \"\"\"\n",
        "        self.PCA.fit(X)\n",
        "        X = self.use_PCA_func(X)\n",
        "\n",
        "        self.sigma = self.sigma_2(X)\n",
        "        self.w = np.random.normal(0, 1/np.sqrt(self.sigma), size=(self.new_dim, self.n_features))\n",
        "        self.b = np.random.uniform(-np.pi, np.pi, size=self.n_features)\n",
        "\n",
        "        new_X = self.new_feat(X)\n",
        "        self.model.fit(new_X, y)\n",
        "        \n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Apply pipeline to obtain scores for input data.\n",
        "        \"\"\"\n",
        "        X = self.use_PCA_func(X)\n",
        "        new_X = self.new_feat(X)\n",
        "        return self.model.predict_proba(new_X) \n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Apply pipeline to obtain discrete predictions for input data.\n",
        "        \"\"\"\n",
        "        X = self.use_PCA_func(X)\n",
        "        new_X = self.new_feat(X)\n",
        "        return self.model.predict(new_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79b2e4be-7542-46b6-ce13-716b893cb9b7",
        "id": "CF9dS-99fJHY"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Выполняется за 43.79927086830139\n",
            "Accuracy: 0.8327\n"
          ]
        }
      ],
      "source": [
        "beg = time.time()\n",
        "\n",
        "sinexp_model = RFFPipeline()\n",
        "sinexp_model.fit(x_train_loc, y_train_loc)\n",
        "\n",
        "print(\"Выполняется за\", time.time() - beg)\n",
        "print(\"Accuracy:\", accuracy_score(sinexp_model.predict(x_test), y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGW39-kXSeoL"
      },
      "outputs": [],
      "source": [
        "# DecisionTree\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class DecisionTreePipeline(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, n_features=1000, new_dim=50, use_PCA=True):\n",
        "        \"\"\"        \n",
        "        Implements pipeline, which consists of PCA decomposition,\n",
        "        Random Fourier Features approximation and linear classification model.\n",
        "        \n",
        "        n_features, int: amount of synthetic random features generated with RFF approximation.\n",
        "\n",
        "        new_dim, int: PCA output size.\n",
        "        \n",
        "        use_PCA, bool: whether to include PCA preprocessing.\n",
        "        \n",
        "        classifier, string: either 'svm' or 'logreg', a linear classification model to use on top of pipeline.\n",
        "        \n",
        "        Feel free to edit this template for your preferences.    \n",
        "        \"\"\"\n",
        "        self.n_features = n_features\n",
        "        self.new_dim = new_dim\n",
        "\n",
        "        self.use_PCA = use_PCA\n",
        "        self.PCA = PCA(n_components=self.new_dim)\n",
        "        self.model = DecisionTreeClassifier()\n",
        "  \n",
        "    def sigma_2(self, X):\n",
        "        # получим миллион пар (i_k, j_k)\n",
        "        i = np.random.choice(X.shape[0], size=1000000)\n",
        "        j = np.random.choice(X.shape[0], size=1000000)\n",
        "\n",
        "        # но некоторые могут быть вида (i, i), их нельзя сравнивать. просто выкинем\n",
        "        # т.к X достаточно большой, сильно на числе пар это не отразится\n",
        "        i, j = i[i!=j], j[i!=j]\n",
        "\n",
        "        # результат\n",
        "        return np.median(np.sum((X[i] - X[j])**2, axis=1))\n",
        "\n",
        "    def use_PCA_func(self, X):\n",
        "        if self.use_PCA:\n",
        "            X = self.PCA.transform(X)\n",
        "        else:\n",
        "            self.new_dim = X.shape[1]\n",
        "        return X\n",
        "\n",
        "    def new_feat(self, X):\n",
        "      return np.cos(X.dot(self.w) + self.b)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit all parts of algorithm (PCA, RFF, Classification) to training set.\n",
        "        \"\"\"\n",
        "        self.PCA.fit(X)\n",
        "        X = self.use_PCA_func(X)\n",
        "\n",
        "        self.sigma = self.sigma_2(X)\n",
        "        self.w = np.random.normal(0, 1/np.sqrt(self.sigma), size=(self.new_dim, self.n_features))\n",
        "        self.b = np.random.uniform(-np.pi, np.pi, size=self.n_features)\n",
        "\n",
        "        new_X = self.new_feat(X)\n",
        "        self.model.fit(new_X, y)\n",
        "        \n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Apply pipeline to obtain scores for input data.\n",
        "        \"\"\"\n",
        "        X = self.use_PCA_func(X)\n",
        "        new_X = self.new_feat(X)\n",
        "        return self.model.predict_proba(new_X) \n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Apply pipeline to obtain discrete predictions for input data.\n",
        "        \"\"\"\n",
        "        X = self.use_PCA_func(X)\n",
        "        new_X = self.new_feat(X)\n",
        "        return self.model.predict(new_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54f32af5-2795-4863-840f-3cb2315464eb",
        "id": "VSvbnlb5fTzH"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Выполняется за 112.11686778068542\n",
            "Accuracy: 0.7508\n"
          ]
        }
      ],
      "source": [
        "beg = time.time()\n",
        "\n",
        "tree_model = DecisionTreePipeline()\n",
        "tree_model.fit(x_train_loc, y_train_loc)\n",
        "\n",
        "print(\"Выполняется за\", time.time() - beg)\n",
        "print(\"Accuracy:\", accuracy_score(tree_model.predict(x_test), y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Z2PC69NSI5s"
      },
      "outputs": [],
      "source": [
        "# RidgeClassifier\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class RidgePipeline(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, n_features=1000, new_dim=50, use_PCA=True):\n",
        "        \"\"\"        \n",
        "        Implements pipeline, which consists of PCA decomposition,\n",
        "        Random Fourier Features approximation and linear classification model.\n",
        "        \n",
        "        n_features, int: amount of synthetic random features generated with RFF approximation.\n",
        "\n",
        "        new_dim, int: PCA output size.\n",
        "        \n",
        "        use_PCA, bool: whether to include PCA preprocessing.\n",
        "        \n",
        "        classifier, string: either 'svm' or 'logreg', a linear classification model to use on top of pipeline.\n",
        "        \n",
        "        Feel free to edit this template for your preferences.    \n",
        "        \"\"\"\n",
        "        self.n_features = n_features\n",
        "        self.new_dim = new_dim\n",
        "\n",
        "        self.use_PCA = use_PCA\n",
        "        self.PCA = PCA(n_components=self.new_dim)\n",
        "        self.model = RidgeClassifier()\n",
        "  \n",
        "    def sigma_2(self, X):\n",
        "        # получим миллион пар (i_k, j_k)\n",
        "        i = np.random.choice(X.shape[0], size=1000000)\n",
        "        j = np.random.choice(X.shape[0], size=1000000)\n",
        "\n",
        "        # но некоторые могут быть вида (i, i), их нельзя сравнивать. просто выкинем\n",
        "        # т.к X достаточно большой, сильно на числе пар это не отразится\n",
        "        i, j = i[i!=j], j[i!=j]\n",
        "\n",
        "        # результат\n",
        "        return np.median(np.sum((X[i] - X[j])**2, axis=1))\n",
        "\n",
        "    def use_PCA_func(self, X):\n",
        "        if self.use_PCA:\n",
        "            X = self.PCA.transform(X)\n",
        "        else:\n",
        "            self.new_dim = X.shape[1]\n",
        "        return X\n",
        "\n",
        "    def new_feat(self, X):\n",
        "      return np.cos(X.dot(self.w) + self.b)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit all parts of algorithm (PCA, RFF, Classification) to training set.\n",
        "        \"\"\"\n",
        "        self.PCA.fit(X)\n",
        "        X = self.use_PCA_func(X)\n",
        "\n",
        "        self.sigma = self.sigma_2(X)\n",
        "        self.w = np.random.normal(0, 1/np.sqrt(self.sigma), size=(self.new_dim, self.n_features))\n",
        "        self.b = np.random.uniform(-np.pi, np.pi, size=self.n_features)\n",
        "\n",
        "        new_X = self.new_feat(X)\n",
        "        self.model.fit(new_X, y)\n",
        "        \n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Apply pipeline to obtain scores for input data.\n",
        "        \"\"\"\n",
        "        X = self.use_PCA_func(X)\n",
        "        new_X = self.new_feat(X)\n",
        "        return self.model.predict_proba(new_X) \n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Apply pipeline to obtain discrete predictions for input data.\n",
        "        \"\"\"\n",
        "        X = self.use_PCA_func(X)\n",
        "        new_X = self.new_feat(X)\n",
        "        return self.model.predict(new_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a63f9bf-811c-4776-e44f-31983447c880",
        "id": "GmCSkxySfYeo"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Выполняется за 8.174607276916504\n",
            "Accuracy: 0.8497\n"
          ]
        }
      ],
      "source": [
        "beg = time.time()\n",
        "\n",
        "ridge_model = RidgePipeline()\n",
        "ridge_model.fit(x_train_loc, y_train_loc)\n",
        "\n",
        "print(\"Выполняется за\", time.time() - beg)\n",
        "print(\"Accuracy:\", accuracy_score(ridge_model.predict(x_test), y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AWblroGVyg5"
      },
      "source": [
        "Все новые признаки придумала такие себе: качество на них не лучше, хотя работают быстрее. Например, для sin + exp получали качество 0.83 за 43 секунды, наши методы дольше работают. Лучше всего по качеству из наших новых вариантов показала себя Ridge-регрессия. К тому же время работы очень маленькое. Нашим исходным методом мы получаем лучший резутат 0.87, но за значительно большее время --- 600 секунд. Так что Ridge --- идеальный баланс между временем работы и качеством."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "HW-8 SVM и ядра.ipynb\"",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}